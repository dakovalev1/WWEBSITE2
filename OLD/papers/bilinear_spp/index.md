title: Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling
date:  30 Dec 2021
abstract: In this paper we study the convex-concave saddle-point problem $\min_x \max_y f(x) + y^T \mA x - g(y)$, where $f(x)$ and $g(y)$ are smooth and convex functions. We propose an Accelerated Primal-Dual Gradient Method (APDG) for solving this problem, achieving (i) an optimal linear convergence rate in the strongly-convex-strongly-concave regime, matching the lower complexity bound (Zhang et al., 2021), and (ii) an accelerated linear convergence rate in the case when only one of the functions $f(x)$ and $g(y)$ is strongly convex or even none of them are. Finally, we obtain a linearly convergent algorithm for the general smooth and convex-concave saddle point problem $\min_x \max_y F(x,y)$ without the requirement of strong convexity or strong concavity.
authors:    Dmitry Kovalev
            Alexander Gasnikov
            Peter Richt√°rik
links: {"PDF": "https://arxiv.org/pdf/2112.15199", "NeurIPS 2022": "https://papers.nips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html", "arXiv" : "https://arxiv.org/abs/2112.15199"}