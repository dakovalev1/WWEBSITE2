title: Fast Linear Convergence of Randomized BFGS
date: 26 Feb 2020
abstract: Since the late 1950's when quasi-Newton methods first appeared, they have become one of the most widely used and efficient algorithmic paradigms for unconstrained optimization. Despite their immense practical success, there is little theory that shows why these methods are so efficient. We provide a semi-local rate of convergence for the randomized BFGS method which can be significantly better than that of gradient descent, finally giving theoretical evidence supporting the superior empirical performance of the method.
authors: Dmitry Kovalev
        Robert M. Gower
        Peter Richt√°rik
        Alexander Rogozin
links: {"PDF": "https://arxiv.org/pdf/2002.11337.pdf", "arXiv" : "https://arxiv.org/abs/2002.11337"}