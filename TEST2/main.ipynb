{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import typing\n",
    "import bs4\n",
    "import pickle\n",
    "import urllib.parse\n",
    "import textwrap\n",
    "import dateparser\n",
    "import jinja2\n",
    "\n",
    "PROXY_URL = \"http://5zipXAuZVPsquwtL:wifi;;;;@proxy.froxy.com:9000\"\n",
    "SCHOLAR_PROFILE = \"qHFA5z4AAAAJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url: str) -> str:\n",
    "    with requests.request(\n",
    "        method=\"GET\",\n",
    "        url=url,\n",
    "        headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "        },\n",
    "        proxies={\"https\": PROXY_URL, \"http\": PROXY_URL},\n",
    "    ) as response:\n",
    "\n",
    "        print(\"url =\", url, \"status =\", response.status_code)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://scholar.google.com/citations?hl=en&user=qHFA5z4AAAAJ&pagesize=100 status = 200\n",
      "45\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Y0pCki6q_DkC title = Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:2osOgNQ5qMEC title = Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:bnK-pcrLprsC title = Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Se3iqnhoufwC title = From local SGD to local fixed-point methods for federated learning\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:IjCSPb-OGe4C title = RSN: randomized subspace Newton\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:8k81kl-MbHgC title = Linearly converging error compensated SGD\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tyk-4Ss8FVUC title = Revisiting stochastic extragradient\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:UebtZRa9Y70C title = Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:MXK_kJrjxJIC title = A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:kNdYIx-mwKoC title = Accelerated methods for saddle-point problem\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:YsMSGLbcyi4C title = Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:KlAtU1dfN6UC title = Decentralized distributed optimization for saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:9ZlFYXVOiuMC title = Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RHpTSmoSYBkC title = Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ULOm3_A8WrAC title = ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:TFP_iSt0sucC title = Optimal algorithms for decentralized stochastic variational inequalities\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:HoB7MX3m0LUC title = Accelerated variance-reduced methods for saddle-point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:GnPB-g6toBAC title = The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:0EnyYjriUFMC title = Towards accelerated rates for distributed optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:zYLM7Y9cAGgC title = Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:nb7KW1ujOQ8C title = IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4TOpqqG69KYC title = An optimal algorithm for strongly convex minimization under affine constraints\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:eQOLeE2rZwMC title = Distributed fixed point methods with compressed iterates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ldfaerwXgEUC title = Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RGFaLdJalmkC title = Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:BqipwSGYUEgC title = The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ufrVoPGSRksC title = Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:LkGwnXOMwfcC title = Fast linear convergence of randomized BFGS\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:7PzlFSSx8tAC title = Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:qjMakFHDy7sC title = Stochastic spectral and conjugate descent methods\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:35N4QoGY0k4C title = Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:NaGl4SEjCO4C title = Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:dshw04ExmUIC title = Decentralized Convex Optimization over Time-Varying Graphs\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pyW8ca7W8N0C title = Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:J_g5lzvAfSwC title = On scaled methods for saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:yD5IFk8b50cC title = Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:_xSYboBqXhAC title = Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:d1gkVwhDpl0C title = A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4OULZ7Gr8RgC title = An optimal algorithm for strongly convex min-min optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:p2g8aNsByqUC title = Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tiz5es2fbqcC title = Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:EUQCXRtRnyEC title = Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:V3AGJWp-ZtQC title = Decentralized Optimization with Coupled Constraints\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:geHnlv5EZngC title = Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pqnbT2bcN3wC title = Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n"
     ]
    }
   ],
   "source": [
    "html = get(\n",
    "    f\"https://scholar.google.com/citations?hl=en&user={SCHOLAR_PROFILE}&pagesize=100\"\n",
    ")\n",
    "soup = bs4.BeautifulSoup(html)\n",
    "paper_list: bs4.ResultSet[bs4.Tag] = soup.find_all(\n",
    "    name=\"tr\", attrs={\"class\": \"gsc_a_tr\"}\n",
    ")\n",
    "\n",
    "print(len(paper_list))\n",
    "for paper in paper_list:\n",
    "    paper_link = paper.find(name=\"a\", attrs={\"class\": \"gsc_a_at\"})\n",
    "    print(\n",
    "        \"url =\",\n",
    "        \"https://scholar.google.com\" + paper_link.attrs[\"href\"],\n",
    "        \"title =\",\n",
    "        paper_link.text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get paper cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_soup2dict(paper_html: str) -> typing.Dict[str, bs4.Tag]:\n",
    "    paper_soup = bs4.BeautifulSoup(paper_html)\n",
    "\n",
    "    key_list: bs4.ResultSet[bs4.Tag] = paper_soup.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_field\"}\n",
    "    )\n",
    "    value_list: bs4.ResultSet[bs4.Tag] = paper_soup.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_value\"}\n",
    "    )\n",
    "\n",
    "    paper_dict: typing.Dict[str, bs4.Tag] = {}\n",
    "    paper_dict[\"Title\"] = paper_soup.find(name=\"div\", attrs={\"id\": \"gsc_oci_title\"})\n",
    "    for key, value in zip(key_list, value_list):\n",
    "        paper_dict[key.text] = value\n",
    "\n",
    "    return paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Y0pCki6q_DkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:2osOgNQ5qMEC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:bnK-pcrLprsC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Se3iqnhoufwC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:IjCSPb-OGe4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:8k81kl-MbHgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tyk-4Ss8FVUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:UebtZRa9Y70C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:MXK_kJrjxJIC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:kNdYIx-mwKoC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:YsMSGLbcyi4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:KlAtU1dfN6UC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:9ZlFYXVOiuMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RHpTSmoSYBkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ULOm3_A8WrAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:TFP_iSt0sucC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:HoB7MX3m0LUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:GnPB-g6toBAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:0EnyYjriUFMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:zYLM7Y9cAGgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:nb7KW1ujOQ8C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4TOpqqG69KYC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:eQOLeE2rZwMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ldfaerwXgEUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RGFaLdJalmkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:BqipwSGYUEgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ufrVoPGSRksC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:LkGwnXOMwfcC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:7PzlFSSx8tAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:qjMakFHDy7sC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:35N4QoGY0k4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:NaGl4SEjCO4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:dshw04ExmUIC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pyW8ca7W8N0C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:J_g5lzvAfSwC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:yD5IFk8b50cC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:_xSYboBqXhAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:d1gkVwhDpl0C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4OULZ7Gr8RgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:p2g8aNsByqUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tiz5es2fbqcC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:EUQCXRtRnyEC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:V3AGJWp-ZtQC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:geHnlv5EZngC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pqnbT2bcN3wC status = 200\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = []\n",
    "for paper in paper_list:\n",
    "    paper_url = (\n",
    "        \"https://scholar.google.com\"\n",
    "        + paper.find(name=\"a\", attrs={\"class\": \"gsc_a_at\"}).attrs[\"href\"]\n",
    "    )\n",
    "    paper_html = get(paper_url)\n",
    "    paper_dict = paper_soup2dict(paper_html)\n",
    "    paper_dict_list.append(paper_dict)\n",
    "pickle.dump(paper_dict_list, open(\"papers.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get paper versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Title', 'Authors', 'Publication date', 'Journal', 'Volume', 'Issue', 'Pages', 'Publisher', 'Description', 'Total citations', 'Scholar articles'])\n",
      "0 Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "1 Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "2 Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "3 From local SGD to local fixed-point methods for federated learning\n",
      "4 RSN: randomized subspace Newton\n",
      "5 Linearly converging error compensated SGD\n",
      "6 Revisiting stochastic extragradient\n",
      "7 Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "8 A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "9 Accelerated methods for saddle-point problem\n",
      "10 Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "11 Decentralized distributed optimization for saddle point problems\n",
      "12 Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "13 Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "14 ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "15 Optimal algorithms for decentralized stochastic variational inequalities\n",
      "16 Accelerated variance-reduced methods for saddle-point problems\n",
      "17 The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "18 Towards accelerated rates for distributed optimization over time-varying networks\n",
      "19 Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "20 IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "21 An optimal algorithm for strongly convex minimization under affine constraints\n",
      "22 Distributed fixed point methods with compressed iterates\n",
      "23 Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "24 Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "25 The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "26 Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "27 Fast linear convergence of randomized BFGS\n",
      "28 Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "29 Stochastic spectral and conjugate descent methods\n",
      "30 Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "31 Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "32 Decentralized Convex Optimization over Time-Varying Graphs\n",
      "33 Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "34 On scaled methods for saddle point problems\n",
      "35 Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "36 Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "37 A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization\n",
      "38 An optimal algorithm for strongly convex min-min optimization\n",
      "39 Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "40 Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "41 Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "42 Decentralized Optimization with Coupled Constraints\n",
      "43 Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "44 Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = pickle.load(\n",
    "    open(\"papers.txt\", \"rb\")\n",
    ")\n",
    "print(paper_dict_list[0].keys())\n",
    "for i in range(len(paper_dict_list)):\n",
    "    print(i, paper_dict_list[i][\"Title\"].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_version_urls(scholar_articles: bs4.Tag) -> typing.List[str]:\n",
    "    merged_snippet_list: bs4.ResultSet[bs4.Tag] = scholar_articles.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_merged_snippet\"}\n",
    "    )\n",
    "\n",
    "    version_url_list: typing.List[str] = []\n",
    "    for merged_snippet in merged_snippet_list:\n",
    "        version_href = merged_snippet.find(name=\"a\").attrs[\"href\"]\n",
    "        version_cluster = urllib.parse.parse_qs(\n",
    "            urllib.parse.urlparse(version_href).query\n",
    "        )[\"cluster\"][0]\n",
    "        version_url = f\"https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster={version_cluster}\"\n",
    "        version_url_list.append(version_url)\n",
    "    return version_url_list\n",
    "\n",
    "\n",
    "def extract_versions(scholar_articles: bs4.Tag) -> typing.List[str]:\n",
    "    url_list = extract_version_urls(scholar_articles)\n",
    "\n",
    "    version_list: typing.List[str] = []\n",
    "    for url in url_list:\n",
    "        soup = bs4.BeautifulSoup(get(url))\n",
    "        versions: bs4.ResultSet[bs4.Tag] = soup.find_all(attrs={\"class\": \"gs_rt\"})\n",
    "        for version in versions:\n",
    "            a = version.find(name=\"a\")\n",
    "            if not a is None:\n",
    "                version_list.append(a.attrs[\"href\"])\n",
    "    return version_list\n",
    "\n",
    "\n",
    "# extract_versions(paper_dict_list[9][\"Scholar articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=10432066948921138844 status = 200\n",
      "Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=7006814588298832736 status = 200\n",
      "Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=1371688885190462102 status = 200\n",
      "From local SGD to local fixed-point methods for federated learning\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15088076385024246021 status = 200\n",
      "RSN: randomized subspace Newton\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=7373789986788030402 status = 200\n",
      "Linearly converging error compensated SGD\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9254067822190880000 status = 200\n",
      "Revisiting stochastic extragradient\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=8653605232017703121 status = 200\n",
      "Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15665175903150090179 status = 200\n",
      "A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3790088210576421356 status = 200\n",
      "Accelerated methods for saddle-point problem\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9373209880233203502 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=4686524615161672519 status = 200\n",
      "Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3002705566416746554 status = 200\n",
      "Decentralized distributed optimization for saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15155949253390165076 status = 200\n",
      "Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=218386955491957145 status = 200\n",
      "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=313086167555561841 status = 200\n",
      "ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=13055424477930940340 status = 200\n",
      "Optimal algorithms for decentralized stochastic variational inequalities\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=16974920721308888306 status = 200\n",
      "Accelerated variance-reduced methods for saddle-point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12130567569437016761 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=11037264009171741499 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=14409392578980627652 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=1257530239269417616 status = 200\n",
      "The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12882769054337015160 status = 200\n",
      "Towards accelerated rates for distributed optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=2324971111724151487 status = 200\n",
      "Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=8964049524700423512 status = 200\n",
      "IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=16969044896100418296 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15707566150174104807 status = 200\n",
      "An optimal algorithm for strongly convex minimization under affine constraints\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15110157264304090339 status = 200\n",
      "Distributed fixed point methods with compressed iterates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9469053425606975651 status = 200\n",
      "Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=6416508855679529689 status = 200\n",
      "Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15617746434035789497 status = 200\n",
      "The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=6878778451294198426 status = 200\n",
      "Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=14069246331553141070 status = 200\n",
      "Fast linear convergence of randomized BFGS\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12043257693440176889 status = 200\n",
      "Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=4252407442233130977 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=18367348657283013424 status = 200\n",
      "Stochastic spectral and conjugate descent methods\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15120261814030278283 status = 200\n",
      "Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=10468598298336511368 status = 200\n",
      "Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9024856644395226101 status = 200\n",
      "Decentralized Convex Optimization over Time-Varying Graphs\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12275752593080095457 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=14256695056462406414 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=16900672409194605727 status = 200\n",
      "Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9454741829138024660 status = 200\n",
      "On scaled methods for saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=6878525993176263563 status = 200\n",
      "Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3040641654477328482 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=4942379545739387070 status = 200\n",
      "Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3502687140939495832 status = 200\n",
      "A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3812646967675013786 status = 200\n",
      "An optimal algorithm for strongly convex min-min optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=13028606441871676013 status = 200\n",
      "Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=10357839628968722372 status = 200\n",
      "Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9027699018424793418 status = 200\n",
      "Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3365804316294644363 status = 200\n",
      "Decentralized Optimization with Coupled Constraints\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=13797128321053821676 status = 200\n",
      "Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=11605182395180550612 status = 200\n",
      "Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=7226585304015966991 status = 200\n"
     ]
    }
   ],
   "source": [
    "for paper_dict in paper_dict_list:\n",
    "    print(paper_dict[\"Title\"].text)\n",
    "    if \"Scholar articles\" in paper_dict.keys():\n",
    "        version_list = extract_versions(paper_dict[\"Scholar articles\"])\n",
    "        paper_dict[\"Version urls\"] = version_list\n",
    "    else:\n",
    "        paper_dict[\"Version urls\"] = []\n",
    "\n",
    "pickle.dump(paper_dict_list, open(\"paper_urls.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Title', 'Authors', 'Publication date', 'Journal', 'Volume', 'Issue', 'Pages', 'Publisher', 'Description', 'Total citations', 'Scholar articles', 'Version urls'])\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = pickle.load(\n",
    "    open(\"paper_urls.txt\", \"rb\")\n",
    ")\n",
    "print(paper_dict_list[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLISHER_NETLOC = set(\n",
    "    [\n",
    "        \"proceedings.neurips.cc\",\n",
    "        \"proceedings.mlr.press\",\n",
    "        \"link.springer.com\",\n",
    "        \"ems.press\",\n",
    "        \"www.sciencedirect.com\",\n",
    "        \"www.tandfonline.com\",\n",
    "        \"crm-en.ics.org.ru\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Paper:\n",
    "    def __init__(self, paper_dict: typing.Dict[str, bs4.Tag]) -> None:\n",
    "        self.title: str = paper_dict[\"Title\"].text\n",
    "        self.authors: str = paper_dict[\"Authors\"].text\n",
    "        self.year: int = dateparser.parse(paper_dict[\"Publication date\"].text).year\n",
    "\n",
    "        self.venue: str = \"\"\n",
    "        self.type: str = \"\"\n",
    "\n",
    "        if \"Journal\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Journal\"].text\n",
    "            if self.venue.lower().startswith(\"arxiv\"):\n",
    "                self.type = \"preprint\"\n",
    "            else:\n",
    "                self.type = \"publication\"\n",
    "        elif \"Conference\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Conference\"].text\n",
    "            self.type = \"publication\"\n",
    "        elif \"Book\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Book\"].text\n",
    "            self.type = \"publication\"\n",
    "        elif \"Institution\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Institution\"].text\n",
    "            self.type = \"thesis\"\n",
    "        else:\n",
    "            raise KeyError(\"unknown paper type\")\n",
    "\n",
    "        self.link: str | None = None\n",
    "        self.openreview: str | None = None\n",
    "        self.arxiv: str | None = None\n",
    "        self.pdf: str | None = None\n",
    "\n",
    "        for url in paper_dict[\"Version urls\"]:\n",
    "            url_parsed: urllib.parse.ParseResult = urllib.parse.urlparse(url)\n",
    "            if self.arxiv is None and url_parsed.netloc == \"arxiv.org\":\n",
    "                self.arxiv = url\n",
    "            if self.openreview is None and url_parsed.netloc == \"openreview.net\":\n",
    "                self.openreview = url\n",
    "            if self.link is None and url_parsed.netloc in PUBLISHER_NETLOC:\n",
    "                self.link = url\n",
    "            if self.pdf is None and url_parsed.path.lower().endswith(\"pdf\"):\n",
    "                self.pdf = url\n",
    "\n",
    "        if self.type == \"preprint\":\n",
    "            self.link = self.arxiv\n",
    "\n",
    "        if not self.arxiv is None:\n",
    "            self.pdf = self.arxiv.replace(\"abs\", \"pdf\")\n",
    "\n",
    "        print(self.title)\n",
    "        for url in paper_dict[\"Version urls\"]:\n",
    "            print(\"\\t\", url)\n",
    "\n",
    "    def to_html(self) -> str:\n",
    "        span_title = bs4.Tag(name=\"span\", attrs={\"class\": \"span-paper-title\"})\n",
    "\n",
    "        if self.link is None:\n",
    "            span_title.append(self.title)\n",
    "        else:\n",
    "            a_title = bs4.Tag(name=\"a\", attrs={\"href\": self.link})\n",
    "            a_title.append(self.title)\n",
    "            span_title.append(a_title)\n",
    "\n",
    "        span = bs4.Tag(name=\"span\")\n",
    "        span.append(f\" ({self.authors}). {self.venue}, {self.year}.\")\n",
    "\n",
    "        links: typing.List[str] = []\n",
    "\n",
    "        if not self.link is None:\n",
    "            a_link = bs4.Tag(name=\"a\", attrs={\"href\": self.link})\n",
    "            a_link.append(\"Link\")\n",
    "            links.append(str(a_link))\n",
    "\n",
    "        # if self.type == \"publication\" and not self.openreview is None:\n",
    "        #     a_openreview = bs4.Tag(name=\"a\", attrs={\"href\": self.openreview})\n",
    "        #     a_openreview.append(\"OpenReview\")\n",
    "        #     links.append(str(a_openreview))\n",
    "\n",
    "        if self.type != \"preprint\" and not self.arxiv is None:\n",
    "            a_arxiv = bs4.Tag(name=\"a\", attrs={\"href\": self.arxiv})\n",
    "            a_arxiv.append(\"arXiv\")\n",
    "            links.append(str(a_arxiv))\n",
    "\n",
    "        if not self.pdf is None:\n",
    "            a_pdf = bs4.Tag(name=\"a\", attrs={\"href\": self.pdf})\n",
    "            a_pdf.append(\"PDF\")\n",
    "            links.append(str(a_pdf))\n",
    "\n",
    "        if len(links):\n",
    "            span.append(\" [\")\n",
    "            span.append(bs4.BeautifulSoup(\", \".join(links)))\n",
    "            span.append(\"].\")\n",
    "\n",
    "        soup = bs4.BeautifulSoup()\n",
    "        soup.append(span_title)\n",
    "        soup.append(span)\n",
    "\n",
    "        print(soup, links)\n",
    "        return soup\n",
    "\n",
    "    def __lt__(self, other: typing.Self) -> bool:\n",
    "        return (\n",
    "            self.year > other.year\n",
    "            or (self.year == other.year and self.venue < other.venue)\n",
    "            or (\n",
    "                self.year == other.year\n",
    "                and self.venue == other.venue\n",
    "                and self.title < other.title\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return '<Paper title=\"{}\" authors=\"{}\" venue=\"{}\" year = {}>'.format(\n",
    "            textwrap.shorten(self.title, width=30, placeholder=\"...\"),\n",
    "            textwrap.shorten(self.authors, width=30, placeholder=\"...\"),\n",
    "            self.venue,\n",
    "            self.year,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "\t https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\n",
      "\t https://repository.kaust.edu.sa/bitstream/10754/653103/1/1904.05115.pdf\n",
      "\t https://dclibrary.mbzuai.ac.ae/mlfp/359/\n",
      "\t https://arxiv.org/abs/1904.05115\n",
      "\t https://www.researchgate.net/profile/Samuel-Horvath-2/publication/332342206_Stochastic_Distributed_Learning_with_Gradient_Quantization_and_Variance_Reduction/links/5d359e724585153e5916c157/Stochastic-Distributed-Learning-with-Gradient-Quantization-and-Variance-Reduction.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190405115H/abstract\n",
      "\t https://2019.ds3-datascience-polytechnique.fr/wp-content/uploads/2019/06/DS3-608_2019.pdf\n",
      "\t https://publications.cispa.saarland/id/eprint/3980\n",
      "\t https://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2019/06/DS3-608_2019.pdf\n",
      "\t https://nchr.elsevierpure.com/en/publications/stochastic-distributed-learning-with-gradient-quantization-and-do\n",
      "\t https://core.ac.uk/download/pdf/599569336.pdf\n",
      "\t https://www.ingentaconnect.com/content/tandf/goms/2023/00000038/00000001/art00004\n",
      "\t https://publications.cispa.de/articles/journal_contribution/Stochastic_distributed_learning_with_gradient_quantization_and_double-variance_reduction/25469572\n",
      "\t https://infoscience.epfl.ch/record/297436\n",
      "\t https://publications.cispa.de/ndownloader/files/45257332\n",
      "\t https://repository.kaust.edu.sa/items/e8e43ddb-362d-415c-9815-95432bbcb200\n",
      "\t https://publications.cispa.saarland/3980/\n",
      "Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "\t https://proceedings.mlr.press/v117/kovalev20a.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190108689K/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/653122\n",
      "\t https://arxiv.org/abs/1901.08689\n",
      "\t http://proceedings.mlr.press/v117/kovalev20a.html\n",
      "\t http://dml.mathdoc.fr/item/1901.08689/\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/cad85c90-bfce-430d-ad1a-f74af61a3f4c/content\n",
      "Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "\t https://arxiv.org/abs/2002.11364\n",
      "\t https://repository.kaust.edu.sa/bitstreams/219e1c40-7bf6-42e0-b059-4c71112b7b66/download\n",
      "\t https://www.researchgate.net/profile/Zhize-Li/publication/339526946_Acceleration_for_Compressed_Gradient_Descent_in_Distributed_and_Federated_Optimization/links/5e59fe2b4585152ce8f85f96/Acceleration-for-Compressed-Gradient-Descent-in-Distributed-and-Federated-Optimization.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200211364L/abstract\n",
      "\t https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-15-19-00UTC-6191-acceleration_fo.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3524938.3525485\n",
      "\t https://ink.library.smu.edu.sg/sis_research/8681/\n",
      "\t https://zhizeli.github.io/files/acgd_slides.pdf\n",
      "\t https://pdfs.semanticscholar.org/bd68/6257ad1f948a30a37c32f91abb388ce62e1e.pdf\n",
      "\t https://elibrary.ru/item.asp?id=45863264\n",
      "\t https://proceedings.mlr.press/v119/li20g.html\n",
      "\t https://icml.cc/media/icml-2020/Slides/6191.pdf\n",
      "\t http://proceedings.mlr.press/v119/li20g.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/662100\n",
      "\t https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?params=/context/sis_research/article/9684/&path_info=ICML20_full_adiana.pdf\n",
      "From local SGD to local fixed-point methods for federated learning\n",
      "\t http://proceedings.mlr.press/v119/malinovskiy20a.html\n",
      "\t https://elibrary.ru/item.asp?id=46742951\n",
      "\t https://repository.kaust.edu.sa/handle/10754/662492\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3524938.3525559\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200401442M/abstract\n",
      "\t http://proceedings.mlr.press/v119/malinovskiy20a/malinovskiy20a-supp.pdf\n",
      "\t https://arxiv.org/abs/2004.01442\n",
      "\t https://www.researchgate.net/profile/Grigory-Malinovskiy/publication/340452734_From_Local_SGD_to_Local_Fixed_Point_Methods_for_Federated_Learning/links/5f0974b145851550509c7be9/From-Local-SGD-to-Local-Fixed-Point-Methods-for-Federated-Learning.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/a75121f4-6d92-4f67-8044-762506afe24b/download\n",
      "RSN: randomized subspace Newton\n",
      "\t https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\n",
      "\t https://arxiv.org/abs/1905.10874\n",
      "\t https://richtarik.org/posters/Poster-RSN.pdf\n",
      "\t https://telecom-paris.hal.science/hal-02365297/\n",
      "\t https://elibrary.ru/item.asp?id=45384452\n",
      "\t http://papers.neurips.cc/paper/8351-rsn-randomized-subspace-newton.pdf\n",
      "\t https://openreview.net/forum?id=ByfmEEreIS\n",
      "\t https://gowerrobert.github.io/pdf/posters/RSN_neurips_poster.pdf\n",
      "\t https://repository.kaust.edu.sa/items/a520cf0c-eb52-4a2a-a786-59de82dc4ec5\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190510874G/abstract\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\n",
      "\t https://hal.science/hal-02365297/\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3454287.3454343\n",
      "\t https://repository.kaust.edu.sa/handle/10754/660275\n",
      "Linearly converging error compensated SGD\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3495724.3497478\n",
      "\t https://pdfs.semanticscholar.org/a6d2/462e8b1777a896058b184923f9129c63a794.pdf\n",
      "\t https://arxiv.org/abs/2010.12292\n",
      "\t https://elibrary.ru/item.asp?id=46790933\n",
      "\t https://www.researchgate.net/profile/Eduard-Gorbunov/publication/344878095_Linearly_Converging_Error_Compensated_SGD/links/61f2bd369a753545e2fe8a69/Linearly-Converging-Error-Compensated-SGD.pdf\n",
      "\t https://proceedings.neurips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666025\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv201012292G/abstract\n",
      "\t https://openreview.net/forum?id=t6y7IjScg2m\n",
      "\t https://eduardgorbunov.github.io/assets/files/neurips_after_party_2021.pdf\n",
      "\t https://eduardgorbunov.github.io/assets/files/ef_sigma_k_poster.pdf\n",
      "Revisiting stochastic extragradient\n",
      "\t http://proceedings.mlr.press/v108/mishchenko20a\n",
      "\t https://repository.kaust.edu.sa/bitstream/handle/10754/660278/Preprintfile1.pdf?sequence=1\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190511373M/abstract\n",
      "\t https://www.academia.edu/download/82489114/1905.11373.pdf\n",
      "\t https://www.konstmish.com/uploads/slides/20-extra.pdf\n",
      "\t http://proceedings.mlr.press/v108/mishchenko20a.html\n",
      "\t https://arxiv.org/abs/1905.11373\n",
      "\t https://repository.kaust.edu.sa/items/38d0a1b0-e999-4edb-8fd7-fb95f0d487e7\n",
      "Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\n",
      "\t https://proceedings.neurips.cc/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\n",
      "\t https://repository.kaust.edu.sa/bitstreams/45db2111-7c3c-4002-bf35-72ccc792492e/download\n",
      "\t https://arxiv.org/abs/2006.11773\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200611773K/abstract\n",
      "\t https://papers.neurips.cc/paper_files/paper/2020/file/d530d454337fb09964237fecb4bea6ce-Paper.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3495724.3497264\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666022\n",
      "A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "\t https://proceedings.mlr.press/v130/kovalev21a.html\n",
      "\t http://proceedings.mlr.press/v130/kovalev21a\n",
      "\t https://arxiv.org/abs/2011.01697\n",
      "\t https://infoscience.epfl.ch/record/286864\n",
      "\t https://richtarik.org/posters/Poster-Decentralized-DIANA.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv201101697K/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/665879\n",
      "\t https://repository.kaust.edu.sa/bitstreams/e5c7d805-a861-4dc8-a17f-cf7d6e58d3d2/download\n",
      "\t https://infoscience.epfl.ch/record/286864?v=pdf\n",
      "Accelerated methods for saddle-point problem\n",
      "\t https://link.springer.com/article/10.1134/S0965542520110020\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666346\n",
      "\t https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=09655425&AN=147479258&h=DqWtxV31KzxKDWR3KUGun43VDY8bDJ%2Bz9%2FS95X7SVsRR8f7JJybjHTMlbrRaetihd%2BenJ8EVa0B6Vx0EnF63Ow%3D%3D&crl=c\n",
      "\t https://www.mathnet.ru/eng/zvmmf11157\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020CMMPh..60.1787A/abstract\n",
      "\t https://elibrary.ru/item.asp?id=45099936\n",
      "\t https://www.mathnet.ru/eng/zvmmf/v60/i11/p1843\n",
      "\t https://arxiv.org/abs/1906.03620\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190603620A/abstract\n",
      "Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "\t https://arxiv.org/abs/1912.01597\n",
      "\t https://repository.kaust.edu.sa/handle/10754/660727\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv191201597K/abstract\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/766f5e70-bb84-4c1b-81ad-efd9c3aaa6b3/content\n",
      "Decentralized distributed optimization for saddle point problems\n",
      "\t https://arxiv.org/abs/2102.07758\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667615\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210207758R/abstract\n",
      "\t https://www.researchgate.net/profile/Pavel-Dvurechensky/publication/349335061_Decentralized_Distributed_Optimization_for_Saddle_Point_Problems/links/6076cc65a03fca55fe29919e/Decentralized-Distributed-Optimization-for-Saddle-Point-Problems.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/ff50fd83-96e5-4515-bd72-0ea1472bb07f/content\n",
      "Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "\t https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\n",
      "\t https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html?roistat_visit=7938444\n",
      "\t https://labmmo.ru/upload/000/u8/3/8/2106-04469.pdf\n",
      "\t https://arxiv.org/abs/2106.04469\n",
      "\t https://papers.neurips.cc/paper/2021/file/bc37e109d92bdc1ea71da6c919d54907-Paper.pdf\n",
      "\t https://fl-icml.github.io/2021/papers/FL-ICML21_paper_46.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210604469K/abstract\n",
      "\t https://proceedings.nips.cc/paper/2021/file/bc37e109d92bdc1ea71da6c919d54907-Paper.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3540261.3541971\n",
      "\t https://elibrary.ru/item.asp?id=49123379\n",
      "\t https://openreview.net/forum?id=L8-54wkift\n",
      "\t https://repository.kaust.edu.sa/handle/10754/669593.1\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/239fe39a-dabf-4148-a5bf-fdc56ef785b1/content\n",
      "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/883f66687a521536c505f9b2fbdcbf1e-Supplemental-Conference.pdf\n",
      "\t https://arxiv.org/abs/2112.15199\n",
      "\t https://repository.kaust.edu.sa/handle/10754/692832\n",
      "\t https://labmmo.ru/upload/000/u8/6/f/2112-15199.pdf\n",
      "\t https://openreview.net/forum?id=FncDhRcRYiN\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv211215199K/abstract\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3601849\n",
      "ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "\t http://proceedings.mlr.press/v139/kovalev21a\n",
      "\t https://shulgin-egor.github.io/files/adom_poster.pdf\n",
      "\t https://richtarik.org/posters/Poster-ADOM-ICML-2021.pdf\n",
      "\t http://proceedings.mlr.press/v139/kovalev21a/kovalev21a-supp.pdf\n",
      "\t https://arxiv.org/abs/2102.09234\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210209234K/abstract\n",
      "\t https://labmmo.ru/upload/000/u8/c/2/2102-09234.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/d0a7e842-7669-49ac-9ada-ae82ce9280f5/download\n",
      "\t https://icml.cc/media/icml-2021/Slides/10125.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667661\n",
      "\t https://shulgin-egor.github.io/publication/adom/adom_poster.pdf\n",
      "Optimal algorithms for decentralized stochastic variational inequalities\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220202771K/abstract\n",
      "\t https://arxiv.org/abs/2202.02771\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3602523\n",
      "\t https://repository.kaust.edu.sa/handle/10754/677972\n",
      "\t https://openreview.net/forum?id=omI5hgwgrsa\n",
      "\t https://labmmo.ru/upload/000/u8/2/2/2202-02771.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/b402bccc-272c-4cfb-acb5-1cb3a2d745f0/content\n",
      "Accelerated variance-reduced methods for saddle-point problems\n",
      "\t https://arxiv.org/abs/2103.09344\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210309344T/abstract\n",
      "\t https://www.academia.edu/download/96798398/2103.09344v2.pdf\n",
      "\t https://labmmo.ru/upload/000/u8/b/a/2103-09344.pdf\n",
      "\t https://www.mathnet.ru/eng/crm1069\n",
      "\t http://crm.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t https://labmmo.ru/upload/000/u8/1/9/22-tominin.pdf\n",
      "\t http://www.crm.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t http://vst.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t http://crm-en.ics.org.ru/journal/article/3325/\n",
      "\t https://www.sciencedirect.com/science/article/pii/S2192440622000247\n",
      "\t https://labmmo.ru/upload/000/u8/6/3/1-s2-0-s2192440622000247-main.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstream/10754/685261/1/1-s2.0-S2192440622000247-main.pdf\n",
      "\t https://oa.tib.eu/renate/handle/123456789/11625\n",
      "\t https://scholar.archive.org/work/sykq2i3kyfcw3gkvw3u5f7grvq/access/wayback/https://oa.tib.eu/renate/bitstream/123456789/11625/1/1-s2-0-S2192440622000247-main.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/94f2de67-b691-4460-a94d-208468c92094/content\n",
      "\t https://oa.tib.eu/renate/backend/api/core/bitstreams/893b3e1f-6e0d-4f5b-8f84-05e137a58338/content\n",
      "\t https://repository.kaust.edu.sa/handle/10754/693497\n",
      "\t https://repository.kaust.edu.sa/items/37b99088-d0fb-4ab9-a25e-31eb3e84198d\n",
      "The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220509647K/abstract\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/e56f394bbd4f0ec81393d767caa5a31b-Supplemental-Conference.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/678185\n",
      "\t https://openreview.net/forum?id=YgmiL2Ur01P\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/e56f394bbd4f0ec81393d767caa5a31b-Supplemental-Conference.pdf\n",
      "\t https://arxiv.org/abs/2205.09647\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3602831\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/b4044a21-effb-419e-83f8-9c6fa796386b/content\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/e56f394bbd4f0ec81393d767caa5a31b-Paper-Conference.pdf\n",
      "Towards accelerated rates for distributed optimization over time-varying networks\n",
      "\t https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\n",
      "\t https://labmmo.ru/upload/000/u8/e/b/2009-11069.pdf\n",
      "\t https://arxiv.org/abs/2009.11069\n",
      "\t https://dl.acm.org/doi/abs/10.1007/978-3-030-91059-4_19\n",
      "\t https://link.springer.com/content/pdf/10.1007/978-3-030-91059-4.pdf#page=272\n",
      "\t https://repository.kaust.edu.sa/handle/10754/665439\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200911069R/abstract\n",
      "\t https://books.google.com/books?hl=en&lr=&id=6HlMEAAAQBAJ&oi=fnd&pg=PA258&ots=aRod_JtBSn&sig=80W5wwG_uN9qqR6v85fypndK55g\n",
      "\t https://elibrary.ru/item.asp?id=47535257\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/11f0c1f2-6dd2-4526-a2cf-0c9e047a3a16/content\n",
      "Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "\t https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\n",
      "\t https://openreview.net/forum?id=HklDsNrlIr\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190511768S/abstract\n",
      "\t https://elibrary.ru/item.asp?id=45347186\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3454287.3454884\n",
      "\t https://adil-salim.github.io/Research/langevin19.pdf\n",
      "\t https://arxiv.org/abs/1905.11768\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\n",
      "\t https://richtarik.org/posters/Poster-SPLA.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstream/handle/10754/660280/Preprintfile1.pdf?sequence=1\n",
      "\t http://papers.neurips.cc/paper/8891-stochastic-proximal-langevin-algorithm-potential-splitting-and-nonasymptotic-rates.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/370aa404-7758-4366-b92f-f73bc2147526/content\n",
      "\t https://adil-salim.github.io/Research/poster-langevin19.pdf\n",
      "IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "\t https://arxiv.org/abs/2102.08374\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667733\n",
      "\t https://www.datascienceassn.org/sites/default/files/IntSGD%20Floatless%20Compression%20of%20Stochastic%20Gradients.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210208374M/abstract\n",
      "\t https://www.konstmish.com/uploads/slides/22-intsgd-slides.pdf\n",
      "\t https://openreview.net/forum?id=pFyXqxChZc\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/5849784f-31de-480c-a3ed-a80e47f2d376/content\n",
      "\t https://repository.kaust.edu.sa/bitstreams/c2660e1d-6c39-4815-8bfe-71587f858327/download\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/c2660e1d-6c39-4815-8bfe-71587f858327/content\n",
      "An optimal algorithm for strongly convex minimization under affine constraints\n",
      "\t https://proceedings.mlr.press/v151/salim22a.html\n",
      "\t https://arxiv.org/abs/2102.11079\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667732\n",
      "\t https://lcondat.github.io/publis/Salim_AISTATS_2022.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210211079S/abstract\n",
      "\t https://repository.kaust.edu.sa/bitstreams/d4e147fb-6317-4dfc-ba50-33777751bda0/download\n",
      "Distributed fixed point methods with compressed iterates\n",
      "\t https://arxiv.org/abs/1912.09925\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv191209925C/abstract\n",
      "\t https://richtarik.org/papers/DFPMCI-new.pdf\n",
      "\t https://richtarik.org/papers/FPMCI.pdf\n",
      "Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "\t https://ems.press/journals/mag/articles/9939904\n",
      "\t https://content.ems.press/assets/public/full-texts/serials/mag/127/9939904/online/10.4171-mag-112.pdf\n",
      "\t https://arxiv.org/abs/2208.13592\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220813592B/abstract\n",
      "\t https://euromathsoc.org/magazine/articles/112\n",
      "\t https://repository.kaust.edu.sa/handle/10754/680934\n",
      "\t https://ems.press/content/serial-article-files/27019\n",
      "\t https://content.ems.press/assets/public/full-texts/serials/mag/127/9939904/online-first/10.4171-mag-112-online-first.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstream/10754/680934.1/1/2208.13592.pdf\n",
      "Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\n",
      "\t https://arxiv.org/abs/2205.15136\n",
      "\t https://labmmo.ru/upload/000/u8/5/3/1749-optimal-gradient-sliding-and-i.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3602697\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/d88f6f81e1aaf606776ffdd06fdf24ef-Paper-Conference.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220515136K/abstract\n",
      "\t https://openreview.net/forum?id=QrK0WDLVHZt\n",
      "\t https://repository.kaust.edu.sa/bitstream/10754/678602/1/2205.15136.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/49b0e2d7-321a-4563-9919-bdbef75a2d03/content\n",
      "The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/5e2ed801f62102f531d109d7c6e1b62f-Supplemental-Conference.pdf\n",
      "\t https://arxiv.org/abs/2205.05653\n",
      "\t https://labmmo.ru/upload/000/u8/f/7/2205-05653.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/677973\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220505653K/abstract\n",
      "\t https://openreview.net/forum?id=pD5Pl5hen_g\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3601338\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/242575fb-ca9c-4dee-b1f5-1eff8963d6b5/content\n",
      "Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "\t https://proceedings.mlr.press/v119/hanzely20b.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666019\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200204670H/abstract\n",
      "\t http://proceedings.mlr.press/v119/hanzely20b/hanzely20b-supp.pdf\n",
      "\t https://arxiv.org/abs/2002.04670\n",
      "\t https://elibrary.ru/item.asp?id=45957891\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3524938.3525316\n",
      "\t https://repository.kaust.edu.sa/bitstreams/7039c4f0-fde4-4d15-af90-d77de5ad3be3/download\n",
      "Fast linear convergence of randomized BFGS\n",
      "\t https://arxiv.org/abs/2002.11337\n",
      "\t https://repository.kaust.edu.sa/handle/10754/662101\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200211337K/abstract\n",
      "\t https://www.researchgate.net/profile/Robert-Gower-2/publication/339526612_Fast_Linear_Convergence_of_Randomized_BFGS/links/5edfa778299bf1d20bdb9443/Fast-Linear-Convergence-of-Randomized-BFGS.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/67773374-789f-4805-9fca-8d719335730e/content\n",
      "Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "\t https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210705957B/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/670330\n",
      "\t https://link.springer.com/content/pdf/10.1007/978-3-030-91059-4.pdf#page=260\n",
      "\t https://dl.acm.org/doi/abs/10.1007/978-3-030-91059-4_18\n",
      "\t https://labmmo.ru/upload/000/u8/5/7/2107-05957.pdf\n",
      "\t https://books.google.com/books?hl=en&lr=&id=6HlMEAAAQBAJ&oi=fnd&pg=PA246&ots=aRod_JtBUj&sig=pruybKc0NF7OU-91woF1xrGzTZw\n",
      "\t https://arxiv.org/abs/2107.05957\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/f87dc34c-5e44-40e4-9209-4c8f8e6887ff/content\n",
      "\t https://repository.kaust.edu.sa/bitstream/10754/670330.1/1/Preprintfile1.pdf\n",
      "Stochastic spectral and conjugate descent methods\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2018arXiv180203703K/abstract\n",
      "\t http://papers.neurips.cc/paper/7596-stochastic-spectral-and-conjugate-descent-methods.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/627183\n",
      "\t https://arxiv.org/abs/1802.03703\n",
      "\t https://www.dmitry-kovalev.com/posts/ssd_talk_march_2019/ssd.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3327144.3327255\n",
      "\t https://www.researchgate.net/profile/Eduard-Gorbunov/publication/323141815_Stochastic_Spectral_and_Conjugate_Descent_Methods/links/5ae987fb45851588dd820f3c/Stochastic-Spectral-and-Conjugate-Descent-Methods.pdf\n",
      "\t https://richtarik.org/posters/Poster-SSCD.pdf\n",
      "\t https://elibrary.ru/item.asp?id=38676976\n",
      "\t https://proceedings.neurips.cc/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/b094154a-c56f-4e82-922a-c020628b4374/content\n",
      "Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Supplemental-Conference.pdf\n",
      "\t https://arxiv.org/abs/2207.03957\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220703957S/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/679905\n",
      "\t https://openreview.net/forum?id=W72rB0wwLVu\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Supplemental-Conference.pdf\n",
      "\t https://richtarik.org/papers/APDA_Inexact_Prox.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3601853\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Paper-Conference.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/d6ec3d8a-67f3-4a37-a713-a3cf0c317e1c/content\n",
      "Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "\t https://link.springer.com/article/10.1007/s10287-023-00485-9\n",
      "\t https://repository.kaust.edu.sa/items/0f028000-e96b-4fd3-9d70-175486c3b56c\n",
      "\t https://ideas.repec.org/a/spr/comgts/v21y2024i1d10.1007_s10287-023-00485-9.html\n",
      "\t https://econpapers.repec.org/article/sprcomgts/v_3a21_3ay_3a2024_3ai_3a1_3ad_3a10.1007_5fs10287-023-00485-9.htm\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220600090M/abstract\n",
      "\t https://arxiv.org/abs/2206.00090\n",
      "Decentralized Convex Optimization over Time-Varying Graphs\n",
      "\t https://link.springer.com/content/pdf/10.1007/978-3-030-54621-2_860-1.pdf\n",
      "\t https://arxiv.org/abs/2210.09719\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv221009719R/abstract\n",
      "Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "\t https://proceedings.mlr.press/v202/metelev23a.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2023arXiv230111817M/abstract\n",
      "\t https://labmmo.ru/upload/000/u8/3/1/2301-11817.pdf\n",
      "\t https://arxiv.org/abs/2301.11817\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3618408.3619429\n",
      "\t http://proceedings.mlr.press/v202/metelev23a/metelev23a.pdf\n",
      "\t https://openreview.net/forum?id=DwDQNKF4oy\n",
      "On scaled methods for saddle point problems\n",
      "\t https://arxiv.org/abs/2206.08303\n",
      "\t https://dclibrary.mbzuai.ac.ae/mlfp/156/\n",
      "\t https://repository.kaust.edu.sa/handle/10754/679188\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220608303B/abstract\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/af3ba327-cd48-4690-8961-d7131807f31b/content\n",
      "Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "\t https://repository.kaust.edu.sa/handle/10754/678603\n",
      "\t https://elibrary.ru/download/elibrary_50045117_14955880.pdf#page=156\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/1b5dd049-1d24-44fb-86f0-6d5277f2ebc8/content\n",
      "\t https://elibrary.ru/download/elibrary_50045117_78643624.pdf#page=156\n",
      "\t https://link.springer.com/article/10.1007/s10287-023-00493-9\n",
      "\t https://arxiv.org/abs/2205.15669\n",
      "\t https://europepmc.org/article/ppr/ppr688087\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220515669Y/abstract\n",
      "\t https://ideas.repec.org/a/spr/comgts/v21y2024i1d10.1007_s10287-023-00493-9.html\n",
      "\t https://econpapers.repec.org/article/sprcomgts/v_3a21_3ay_3a2024_3ai_3a1_3ad_3a10.1007_5fs10287-023-00493-9.htm\n",
      "\t https://www.researchsquare.com/article/rs-3126039/latest\n",
      "Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "\t https://link.springer.com/article/10.1007/s10287-023-00479-7\n",
      "\t https://labmmo.ru/upload/000/u8/c/8/2307-00392.pdf\n",
      "\t https://www.researchgate.net/profile/Aleksandr-Lobanov-6/publication/372074109_Non-Smooth_Setting_of_Stochastic_Decentralized_Convex_Optimization_Problem_Over_Time-Varying_Graphs/links/64cba034d394182ab3a10747/Non-Smooth-Setting-of-Stochastic-Decentralized-Convex-Optimization-Problem-Over-Time-Varying-Graphs.pdf\n",
      "\t https://econpapers.repec.org/article/sprcomgts/v_3a20_3ay_3a2023_3ai_3a1_3ad_3a10.1007_5fs10287-023-00479-7.htm\n",
      "\t https://ideas.repec.org/a/spr/comgts/v20y2023i1d10.1007_s10287-023-00479-7.html\n",
      "\t https://arxiv.org/abs/2307.00392\n",
      "\t https://ui.adsabs.harvard.edu/abs/2023arXiv230700392L/abstract\n",
      "A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization\n",
      "\t http://crm-en.ics.org.ru/journal/article/2685/\n",
      "\t https://www.mathnet.ru/eng/crm253\n",
      "\t http://crm-en.ics.org.ru/journal/article/references/2685/\n",
      "An optimal algorithm for strongly convex min-min optimization\n",
      "\t https://arxiv.org/abs/2212.14439\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv221214439G/abstract\n",
      "\t https://repository.kaust.edu.sa/items/38439283-3258-40d9-a35a-aeb3be0eb73f\n",
      "Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "\t https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\n",
      "\t https://repository.kaust.edu.sa/handle/10754/696834\n",
      "\t https://repository.kaust.edu.sa/items/972f8f84-000d-409a-9a3e-fbae68b9bd4a\n",
      "Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "\t https://arxiv.org/abs/2402.02490\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv240202490M/abstract\n",
      "Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "\t https://arxiv.org/abs/2307.12946\n",
      "\t https://ui.adsabs.harvard.edu/abs/2023arXiv230712946B/abstract\n",
      "Decentralized Optimization with Coupled Constraints\n",
      "\t https://arxiv.org/abs/2407.02020\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv240702020Y/abstract\n",
      "Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "\t https://arxiv.org/abs/2405.18031\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv240518031K/abstract\n",
      "Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n",
      "\t https://repository.kaust.edu.sa/handle/10754/682331\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/115f71a4-0e9d-4dfe-9f09-a1434b75e39e/content\n"
     ]
    }
   ],
   "source": [
    "def split_by_year(\n",
    "    paper_list: typing.List[Paper], preprint: bool\n",
    ") -> typing.List[typing.Tuple[int, Paper]]:\n",
    "    paper_list.sort(reverse=preprint)\n",
    "    pd: typing.Dict[int, Paper] = {}\n",
    "    for paper in paper_list:\n",
    "        if paper.year in pd.keys():\n",
    "            pd[paper.year].append(paper)\n",
    "        else:\n",
    "            pd[paper.year] = [paper]\n",
    "    result = [(k, v) for k, v in pd.items()]\n",
    "    result.sort(reverse=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "paper_list: typing.List[Paper] = []\n",
    "publication_list = []\n",
    "preprint_list = []\n",
    "for paper_dict in paper_dict_list:\n",
    "    paper = Paper(paper_dict)\n",
    "    if paper.type == \"publication\":\n",
    "        publication_list.append(paper)\n",
    "    elif paper.type == \"preprint\":\n",
    "        preprint_list.append(paper)\n",
    "\n",
    "publication_list = split_by_year(publication_list, False)\n",
    "preprint_list = split_by_year(preprint_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1007/s10287-023-00493-9\">Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters</a></span><span> (Olga Yufereva, Michael Persiianov, Pavel Dvurechensky, Alexander Gasnikov, Dmitry Kovalev). Computational Management Science, 2024. [<html><body><a href=\"https://link.springer.com/article/10.1007/s10287-023-00493-9\">Link</a>, <a href=\"https://arxiv.org/abs/2205.15669\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2205.15669\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/article/10.1007/s10287-023-00493-9\">Link</a>', '<a href=\"https://arxiv.org/abs/2205.15669\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2205.15669\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1007/s10287-023-00485-9\">Decentralized saddle-point problems with different constants of strong convexity and strong concavity</a></span><span> (Dmitry Metelev, Alexander Rogozin, Alexander Gasnikov, Dmitry Kovalev). Computational Management Science, 2024. [<html><body><a href=\"https://link.springer.com/article/10.1007/s10287-023-00485-9\">Link</a>, <a href=\"https://arxiv.org/abs/2206.00090\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2206.00090\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/article/10.1007/s10287-023-00485-9\">Link</a>', '<a href=\"https://arxiv.org/abs/2206.00090\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2206.00090\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\">Decentralized saddle point problems via non-Euclidean mirror prox</a></span><span> (Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, Dmitry Kovalev, Pavel Dvurechensky, Alexander Gasnikov). Optimization Methods and Software, 2024. [<html><body><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\">Link</a></body></html>].</span> ['<a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\">Link</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1007/s10287-023-00479-7\">Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs</a></span><span> (Aleksandr Lobanov, Andrew Veprikov, Georgiy Konin, Aleksandr Beznosikov, Alexander Gasnikov, Dmitry Kovalev). Computational Management Science, 2023. [<html><body><a href=\"https://link.springer.com/article/10.1007/s10287-023-00479-7\">Link</a>, <a href=\"https://arxiv.org/abs/2307.00392\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2307.00392\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/article/10.1007/s10287-023-00479-7\">Link</a>', '<a href=\"https://arxiv.org/abs/2307.00392\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2307.00392\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/content/pdf/10.1007/978-3-030-54621-2_860-1.pdf\">Decentralized Convex Optimization over Time-Varying Graphs</a></span><span> (Alexander Rogozin, Alexander Gasnikov, Aleksander Beznosikov, Dmitry Kovalev). Encyclopedia of Optimization, 2023. [<html><body><a href=\"https://link.springer.com/content/pdf/10.1007/978-3-030-54621-2_860-1.pdf\">Link</a>, <a href=\"https://arxiv.org/abs/2210.09719\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2210.09719\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/content/pdf/10.1007/978-3-030-54621-2_860-1.pdf\">Link</a>', '<a href=\"https://arxiv.org/abs/2210.09719\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2210.09719\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://ems.press/journals/mag/articles/9939904\">Smooth monotone stochastic variational inequalities and saddle point problems: A survey</a></span><span> (Aleksandr Beznosikov, Boris Polyak, Eduard Gorbunov, Dmitry Kovalev, Alexander Gasnikov). European Mathematical Society Magazine, 2023. [<html><body><a href=\"https://ems.press/journals/mag/articles/9939904\">Link</a>, <a href=\"https://arxiv.org/abs/2208.13592\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2208.13592\">PDF</a></body></html>].</span> ['<a href=\"https://ems.press/journals/mag/articles/9939904\">Link</a>', '<a href=\"https://arxiv.org/abs/2208.13592\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2208.13592\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v202/metelev23a.html\">Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?</a></span><span> (Dmitry Metelev, Alexander Rogozin, Dmitry Kovalev, Alexander Gasnikov). International Conference on Machine Learning, 2023. [<html><body><a href=\"https://proceedings.mlr.press/v202/metelev23a.html\">Link</a>, <a href=\"https://arxiv.org/abs/2301.11817\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2301.11817\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.mlr.press/v202/metelev23a.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2301.11817\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2301.11817\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\">Stochastic distributed learning with gradient quantization and double-variance reduction</a></span><span> (Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Peter Richtárik, Sebastian Stich). Optimization Methods and Software, 2023. [<html><body><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\">Link</a>, <a href=\"https://arxiv.org/abs/1904.05115\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1904.05115\">PDF</a></body></html>].</span> ['<a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\">Link</a>', '<a href=\"https://arxiv.org/abs/1904.05115\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1904.05115\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\">Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling</a></span><span> (Dmitry Kovalev, Alexander Gasnikov, Peter Richtárik). Advances in Neural Information Processing Systems, 2022. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\">Link</a>, <a href=\"https://arxiv.org/abs/2112.15199\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2112.15199\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2112.15199\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2112.15199\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\">Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox</a></span><span> (Abdurakhmon Sadiev, Dmitry Kovalev, Peter Richtárik). Advances in Neural Information Processing Systems, 2022. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\">Link</a>, <a href=\"https://arxiv.org/abs/2207.03957\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2207.03957\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2207.03957\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2207.03957\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\">Optimal algorithms for decentralized stochastic variational inequalities</a></span><span> (Dmitry Kovalev, Aleksandr Beznosikov, Abdurakhmon Sadiev, Michael Persiianov, Peter Richtárik, Alexander Gasnikov). Advances in Neural Information Processing Systems, 2022. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\">Link</a>, <a href=\"https://arxiv.org/abs/2202.02771\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2202.02771\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2202.02771\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2202.02771\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\">Optimal gradient sliding and its application to optimal distributed optimization under similarity</a></span><span> (Dmitry Kovalev, Aleksandr Beznosikov, Ekaterina Borodich, Alexander Gasnikov, Gesualdo Scutari). Advances in Neural Information Processing Systems, 2022. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\">Link</a>, <a href=\"https://arxiv.org/abs/2205.15136\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2205.15136\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2205.15136\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2205.15136\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\">The first optimal acceleration of high-order methods in smooth convex optimization</a></span><span> (Dmitry Kovalev, Alexander Gasnikov). Advances in Neural Information Processing Systems, 2022. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\">Link</a>, <a href=\"https://arxiv.org/abs/2205.09647\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2205.09647\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2205.09647\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2205.09647\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\">The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization</a></span><span> (Dmitry Kovalev, Alexander Gasnikov). Advances in Neural Information Processing Systems, 2022. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\">Link</a>, <a href=\"https://arxiv.org/abs/2205.05653\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2205.05653\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2205.05653\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2205.05653\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\">Accelerated variance-reduced methods for saddle-point problems</a></span><span> (Ekaterina Borodich, Vladislav Tominin, Yaroslav Tominin, Dmitry Kovalev, Alexander Gasnikov, Pavel Dvurechensky). EURO Journal on Computational Optimization, 2022. [<html><body><a href=\"http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\">Link</a>, <a href=\"https://arxiv.org/abs/2103.09344\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2103.09344\">PDF</a></body></html>].</span> ['<a href=\"http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\">Link</a>', '<a href=\"https://arxiv.org/abs/2103.09344\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2103.09344\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v151/salim22a.html\">An optimal algorithm for strongly convex minimization under affine constraints</a></span><span> (Adil Salim, Laurent Condat, Dmitry Kovalev, Peter Richtárik). International Conference on Artificial Intelligence and Statistics, 2022. [<html><body><a href=\"https://proceedings.mlr.press/v151/salim22a.html\">Link</a>, <a href=\"https://arxiv.org/abs/2102.11079\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2102.11079\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.mlr.press/v151/salim22a.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2102.11079\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2102.11079\">PDF</a>']\n",
      "<span class=\"span-paper-title\">IntSGD: Adaptive floatless compression of stochastic gradients</span><span> (Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, Peter Richtárik). International Conference on Learning Representations, 2022. [<html><body><a href=\"https://arxiv.org/abs/2102.08374\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2102.08374\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2102.08374\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2102.08374\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\">Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks</a></span><span> (Dmitry Kovalev, Elnur Gasanov, Alexander Gasnikov, Peter Richtarik). Advances in Neural Information Processing Systems, 2021. [<html><body><a href=\"https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\">Link</a>, <a href=\"https://arxiv.org/abs/2106.04469\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2106.04469\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2106.04469\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2106.04469\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v130/kovalev21a.html\">A linearly convergent algorithm for decentralized optimization: Sending less bits for free!</a></span><span> (Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, Sebastian Stich). International Conference on Artificial Intelligence and Statistics, 2021. [<html><body><a href=\"https://proceedings.mlr.press/v130/kovalev21a.html\">Link</a>, <a href=\"https://arxiv.org/abs/2011.01697\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2011.01697\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.mlr.press/v130/kovalev21a.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2011.01697\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2011.01697\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"http://proceedings.mlr.press/v139/kovalev21a\">ADOM: accelerated decentralized optimization method for time-varying networks</a></span><span> (Dmitry Kovalev, Egor Shulgin, Peter Richtárik, Alexander V Rogozin, Alexander Gasnikov). International Conference on Machine Learning, 2021. [<html><body><a href=\"http://proceedings.mlr.press/v139/kovalev21a\">Link</a>, <a href=\"https://arxiv.org/abs/2102.09234\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2102.09234\">PDF</a></body></html>].</span> ['<a href=\"http://proceedings.mlr.press/v139/kovalev21a\">Link</a>', '<a href=\"https://arxiv.org/abs/2102.09234\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2102.09234\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\">Near-optimal decentralized algorithms for saddle point problems over time-varying networks</a></span><span> (Aleksandr Beznosikov, Alexander Rogozin, Dmitry Kovalev, Alexander Gasnikov). Optimization and Applications: 12th International Conference, OPTIMA 2021, Petrovac, Montenegro, September 27–October 1, 2021, Proceedings 12, 2021. [<html><body><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\">Link</a>, <a href=\"https://arxiv.org/abs/2107.05957\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2107.05957\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\">Link</a>', '<a href=\"https://arxiv.org/abs/2107.05957\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2107.05957\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\">Towards accelerated rates for distributed optimization over time-varying networks</a></span><span> (Alexander Rogozin, Vladislav Lukoshkin, Alexander Gasnikov, Dmitry Kovalev, Egor Shulgin). Optimization and Applications: 12th International Conference, OPTIMA 2021, Petrovac, Montenegro, September 27–October 1, 2021, Proceedings 12, 2021. [<html><body><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\">Link</a>, <a href=\"https://arxiv.org/abs/2009.11069\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2009.11069\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\">Link</a>', '<a href=\"https://arxiv.org/abs/2009.11069\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2009.11069\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\">Linearly converging error compensated SGD</a></span><span> (Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, Peter Richtárik). Advances in Neural Information Processing Systems, 2020. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\">Link</a>, <a href=\"https://arxiv.org/abs/2010.12292\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2010.12292\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2010.12292\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2010.12292\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\">Optimal and practical algorithms for smooth and strongly convex decentralized optimization</a></span><span> (Dmitry Kovalev, Adil Salim, Peter Richtárik). Advances in Neural Information Processing Systems, 2020. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\">Link</a>, <a href=\"https://arxiv.org/abs/2006.11773\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2006.11773\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2006.11773\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2006.11773\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v117/kovalev20a.html\">Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop</a></span><span> (Dmitry Kovalev, Samuel Horváth, Peter Richtárik). Algorithmic Learning Theory, 2020. [<html><body><a href=\"https://proceedings.mlr.press/v117/kovalev20a.html\">Link</a>, <a href=\"https://arxiv.org/abs/1901.08689\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1901.08689\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.mlr.press/v117/kovalev20a.html\">Link</a>', '<a href=\"https://arxiv.org/abs/1901.08689\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1901.08689\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1134/S0965542520110020\">Accelerated methods for saddle-point problem</a></span><span> (Mohammad S Alkousa, Alexander Vladimirovich Gasnikov, Darina Mikhailovna Dvinskikh, Dmitry A Kovalev, Fedor Sergeevich Stonyakin). Computational Mathematics and Mathematical Physics, 2020. [<html><body><a href=\"https://link.springer.com/article/10.1134/S0965542520110020\">Link</a>, <a href=\"https://arxiv.org/abs/1906.03620\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1906.03620\">PDF</a></body></html>].</span> ['<a href=\"https://link.springer.com/article/10.1134/S0965542520110020\">Link</a>', '<a href=\"https://arxiv.org/abs/1906.03620\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1906.03620\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"http://proceedings.mlr.press/v108/mishchenko20a\">Revisiting stochastic extragradient</a></span><span> (Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtárik, Yura Malitsky). International Conference on Artificial Intelligence and Statistics, 2020. [<html><body><a href=\"http://proceedings.mlr.press/v108/mishchenko20a\">Link</a>, <a href=\"https://arxiv.org/abs/1905.11373\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1905.11373\">PDF</a></body></html>].</span> ['<a href=\"http://proceedings.mlr.press/v108/mishchenko20a\">Link</a>', '<a href=\"https://arxiv.org/abs/1905.11373\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1905.11373\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v119/li20g.html\">Acceleration for compressed gradient descent in distributed and federated optimization</a></span><span> (Zhize Li, Dmitry Kovalev, Xun Qian, Peter Richtarik). International Conference on Machine Learning, 2020. [<html><body><a href=\"https://proceedings.mlr.press/v119/li20g.html\">Link</a>, <a href=\"https://arxiv.org/abs/2002.11364\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2002.11364\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.mlr.press/v119/li20g.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2002.11364\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2002.11364\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"http://proceedings.mlr.press/v119/malinovskiy20a.html\">From local SGD to local fixed-point methods for federated learning</a></span><span> (Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, Peter Richtarik). International Conference on Machine Learning, 2020. [<html><body><a href=\"http://proceedings.mlr.press/v119/malinovskiy20a.html\">Link</a>, <a href=\"https://arxiv.org/abs/2004.01442\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2004.01442\">PDF</a></body></html>].</span> ['<a href=\"http://proceedings.mlr.press/v119/malinovskiy20a.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2004.01442\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2004.01442\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v119/hanzely20b.html\">Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems</a></span><span> (Filip Hanzely, Dmitry Kovalev, Peter Richtarik). International Conference on Machine Learning, 2020. [<html><body><a href=\"https://proceedings.mlr.press/v119/hanzely20b.html\">Link</a>, <a href=\"https://arxiv.org/abs/2002.04670\">arXiv</a>, <a href=\"https://arxiv.org/pdf/2002.04670\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.mlr.press/v119/hanzely20b.html\">Link</a>', '<a href=\"https://arxiv.org/abs/2002.04670\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/2002.04670\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\">RSN: randomized subspace Newton</a></span><span> (Robert Gower, Dmitry Kovalev, Felix Lieder, Peter Richtárik). Advances in Neural Information Processing Systems, 2019. [<html><body><a href=\"https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\">Link</a>, <a href=\"https://arxiv.org/abs/1905.10874\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1905.10874\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\">Link</a>', '<a href=\"https://arxiv.org/abs/1905.10874\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1905.10874\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\">Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates</a></span><span> (Adil Salim, Dmitry Kovalev, Peter Richtárik). Advances in Neural Information Processing Systems, 2019. [<html><body><a href=\"https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\">Link</a>, <a href=\"https://arxiv.org/abs/1905.11768\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1905.11768\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\">Link</a>', '<a href=\"https://arxiv.org/abs/1905.11768\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1905.11768\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\">Stochastic spectral and conjugate descent methods</a></span><span> (Dmitry Kovalev, Peter Richtárik, Eduard Gorbunov, Elnur Gasanov). Advances in Neural Information Processing Systems, 2018. [<html><body><a href=\"https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\">Link</a>, <a href=\"https://arxiv.org/abs/1802.03703\">arXiv</a>, <a href=\"https://arxiv.org/pdf/1802.03703\">PDF</a></body></html>].</span> ['<a href=\"https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\">Link</a>', '<a href=\"https://arxiv.org/abs/1802.03703\">arXiv</a>', '<a href=\"https://arxiv.org/pdf/1802.03703\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"http://crm-en.ics.org.ru/journal/article/2685/\">A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization</a></span><span> (Alexander Gasnikov, Dmitry Kovalev). Computer research and modeling, 2018. [<html><body><a href=\"http://crm-en.ics.org.ru/journal/article/2685/\">Link</a></body></html>].</span> ['<a href=\"http://crm-en.ics.org.ru/journal/article/2685/\">Link</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2407.02020\">Decentralized Optimization with Coupled Constraints</a></span><span> (Demyan Yarmoshik, Dmitry Kovalev, Alexander Rogozin, Nikita Kiselev, Daniil Dorin, Alexander Gasnikov). arXiv preprint arXiv:2407.02020, 2024. [<html><body><a href=\"https://arxiv.org/abs/2407.02020\">Link</a>, <a href=\"https://arxiv.org/pdf/2407.02020\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2407.02020\">Link</a>', '<a href=\"https://arxiv.org/pdf/2407.02020\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2405.18031\">Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks</a></span><span> (Dmitry Kovalev, Ekaterina Borodich, Alexander Gasnikov, Dmitrii Feoktistov). arXiv preprint arXiv:2405.18031, 2024. [<html><body><a href=\"https://arxiv.org/abs/2405.18031\">Link</a>, <a href=\"https://arxiv.org/pdf/2405.18031\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2405.18031\">Link</a>', '<a href=\"https://arxiv.org/pdf/2405.18031\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2402.02490\">Decentralized Finite-Sum Optimization over Time-Varying Networks</a></span><span> (Dmitry Metelev, Savelii Chezhegov, Alexander Rogozin, Dmitry Kovalev, Aleksandr Beznosikov, Alexander Sholokhov, Alexander Gasnikov). arXiv preprint arXiv:2402.02490, 2024. [<html><body><a href=\"https://arxiv.org/abs/2402.02490\">Link</a>, <a href=\"https://arxiv.org/pdf/2402.02490\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2402.02490\">Link</a>', '<a href=\"https://arxiv.org/pdf/2402.02490\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2307.12946\">Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems</a></span><span> (Ekaterina Borodich, Georgiy Kormakov, Dmitry Kovalev, Aleksandr Beznosikov, Alexander Gasnikov). arXiv preprint arXiv:2307.12946, 2023. [<html><body><a href=\"https://arxiv.org/abs/2307.12946\">Link</a>, <a href=\"https://arxiv.org/pdf/2307.12946\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2307.12946\">Link</a>', '<a href=\"https://arxiv.org/pdf/2307.12946\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2212.14439\">An optimal algorithm for strongly convex min-min optimization</a></span><span> (Alexander Gasnikov, Dmitry Kovalev, Grigory Malinovsky). arXiv preprint arXiv:2212.14439, 2022. [<html><body><a href=\"https://arxiv.org/abs/2212.14439\">Link</a>, <a href=\"https://arxiv.org/pdf/2212.14439\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2212.14439\">Link</a>', '<a href=\"https://arxiv.org/pdf/2212.14439\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2206.08303\">On scaled methods for saddle point problems</a></span><span> (Aleksandr Beznosikov, Aibek Alanov, Dmitry Kovalev, Martin Takáč, Alexander Gasnikov). arXiv preprint arXiv:2206.08303, 2022. [<html><body><a href=\"https://arxiv.org/abs/2206.08303\">Link</a>, <a href=\"https://arxiv.org/pdf/2206.08303\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2206.08303\">Link</a>', '<a href=\"https://arxiv.org/pdf/2206.08303\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2102.07758\">Decentralized distributed optimization for saddle point problems</a></span><span> (Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, Dmitry Kovalev, Pavel Dvurechensky, Alexander Gasnikov). arXiv preprint arXiv:2102.07758, 2021. [<html><body><a href=\"https://arxiv.org/abs/2102.07758\">Link</a>, <a href=\"https://arxiv.org/pdf/2102.07758\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2102.07758\">Link</a>', '<a href=\"https://arxiv.org/pdf/2102.07758\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/2002.11337\">Fast linear convergence of randomized BFGS</a></span><span> (Dmitry Kovalev, Robert M Gower, Peter Richtárik, Alexander Rogozin). arXiv preprint arXiv:2002.11337, 2020. [<html><body><a href=\"https://arxiv.org/abs/2002.11337\">Link</a>, <a href=\"https://arxiv.org/pdf/2002.11337\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/2002.11337\">Link</a>', '<a href=\"https://arxiv.org/pdf/2002.11337\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/1912.09925\">Distributed fixed point methods with compressed iterates</a></span><span> (Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Peter Richtárik, Adil Salim, Martin Takáč). arXiv preprint arXiv:1912.09925, 2019. [<html><body><a href=\"https://arxiv.org/abs/1912.09925\">Link</a>, <a href=\"https://arxiv.org/pdf/1912.09925\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/1912.09925\">Link</a>', '<a href=\"https://arxiv.org/pdf/1912.09925\">PDF</a>']\n",
      "<span class=\"span-paper-title\"><a href=\"https://arxiv.org/abs/1912.01597\">Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates</a></span><span> (Dmitry Kovalev, Konstantin Mishchenko, Peter Richtárik). arXiv preprint arXiv:1912.01597, 2019. [<html><body><a href=\"https://arxiv.org/abs/1912.01597\">Link</a>, <a href=\"https://arxiv.org/pdf/1912.01597\">PDF</a></body></html>].</span> ['<a href=\"https://arxiv.org/abs/1912.01597\">Link</a>', '<a href=\"https://arxiv.org/pdf/1912.01597\">PDF</a>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76324"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader = jinja2.FileSystemLoader(\"\")\n",
    "env = jinja2.Environment(loader=file_loader)\n",
    "template = env.get_template(\"template_index.html\")\n",
    "\n",
    "output = template.render(\n",
    "    publications=publication_list,\n",
    "    preprints=preprint_list,\n",
    ")\n",
    "open(\"index.html\", \"w\").write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
