title: Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs
date: 1 Jul 2023
abstract: Distributed optimization has a rich history. It has demonstrated its effectiveness in many machine learning applications, etc. In this paper we study a subclass of distributed optimization, namely decentralized optimization in a non-smooth setting. Decentralized means that m agents (machines) working in parallel on one problem communicate only with the neighbors agents (machines), i.e. there is no (central) server through which agents communicate. And by non-smooth setting we mean that each agent has a convex stochastic non-smooth function, that is, agents can hold and communicate information only about the value of the objective function, which corresponds to a gradient-free oracle. In this paper, to minimize the global objective function, which consists of the sum of the functions of each agent, we create a gradient-free algorithm by applying a smoothing scheme via l2 randomization. We also verify in experiments the obtained theoretical convergence results of the gradient-free algorithm proposed in this paper.
authors:        Aleksandr Lobanov
                Andrew Veprikov
                Georgiy Konin
                Aleksandr Beznosikov
                Alexander Gasnikov
                Dmitry Kovalev
links: {"PDF": "https://arxiv.org/pdf/2307.00392", "Computational Management Science" : "https://link.springer.com/article/10.1007/s10287-023-00479-7", "arXiv" : "https://arxiv.org/abs/2307.00392"}