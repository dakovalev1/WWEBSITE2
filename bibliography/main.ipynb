{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import typing\n",
    "import bs4\n",
    "import pickle\n",
    "import urllib.parse\n",
    "import textwrap\n",
    "import dateparser\n",
    "\n",
    "PROXY_URL = \"http://5zipXAuZVPsquwtL:wifi;;;;@proxy.froxy.com:9000\"\n",
    "SCHOLAR_PROFILE = \"qHFA5z4AAAAJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url: str) -> str:\n",
    "    with requests.request(\n",
    "        method=\"GET\",\n",
    "        url=url,\n",
    "        headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "        },\n",
    "        proxies={\"https\": PROXY_URL, \"http\": PROXY_URL},\n",
    "    ) as response:\n",
    "\n",
    "        print(\"url =\", url, \"status =\", response.status_code)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://scholar.google.com/citations?hl=en&user=qHFA5z4AAAAJ&pagesize=100 status = 200\n",
      "47\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Y0pCki6q_DkC title = Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:2osOgNQ5qMEC title = Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:bnK-pcrLprsC title = Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Se3iqnhoufwC title = From local SGD to local fixed-point methods for federated learning\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:IjCSPb-OGe4C title = RSN: randomized subspace Newton\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:8k81kl-MbHgC title = Linearly converging error compensated SGD\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tyk-4Ss8FVUC title = Revisiting stochastic extragradient\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:UebtZRa9Y70C title = Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:MXK_kJrjxJIC title = A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:kNdYIx-mwKoC title = Accelerated methods for saddle-point problem\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:YsMSGLbcyi4C title = Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:KlAtU1dfN6UC title = Decentralized distributed optimization for saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:9ZlFYXVOiuMC title = Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RHpTSmoSYBkC title = Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ULOm3_A8WrAC title = ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:TFP_iSt0sucC title = Optimal algorithms for decentralized stochastic variational inequalities\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:iH-uZ7U-co4C title = On accelerated methods for saddle-point problems with composite structure\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:GnPB-g6toBAC title = The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:0EnyYjriUFMC title = Towards accelerated rates for distributed optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:zYLM7Y9cAGgC title = Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:nb7KW1ujOQ8C title = IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4TOpqqG69KYC title = An optimal algorithm for strongly convex minimization under affine constraints\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:eQOLeE2rZwMC title = Distributed fixed point methods with compressed iterates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ldfaerwXgEUC title = Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RGFaLdJalmkC title = Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:BqipwSGYUEgC title = The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ufrVoPGSRksC title = Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:LkGwnXOMwfcC title = Fast linear convergence of randomized BFGS\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:7PzlFSSx8tAC title = Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:qjMakFHDy7sC title = Stochastic spectral and conjugate descent methods\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:35N4QoGY0k4C title = Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:NaGl4SEjCO4C title = Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ZHo1McVdvXMC title = Decentralized optimization over time-varying graphs: a survey\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pyW8ca7W8N0C title = Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:J_g5lzvAfSwC title = On scaled methods for saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:yD5IFk8b50cC title = Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:_xSYboBqXhAC title = Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:d1gkVwhDpl0C title = A hypothesis about the rate of global convergence for optimal methods (Newtonâ€™s type) in smooth convex optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4OULZ7Gr8RgC title = An optimal algorithm for strongly convex min-min optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:p2g8aNsByqUC title = Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tiz5es2fbqcC title = Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:EUQCXRtRnyEC title = Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:HoB7MX3m0LUC title = Accelerated variance-reduced methods for saddle-point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:V3AGJWp-ZtQC title = Decentralized Optimization with Coupled Constraints\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:geHnlv5EZngC title = Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:dshw04ExmUIC title = Decentralized Convex Optimization over Time-Varying Graphs\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pqnbT2bcN3wC title = Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n"
     ]
    }
   ],
   "source": [
    "html = get(\n",
    "    f\"https://scholar.google.com/citations?hl=en&user={SCHOLAR_PROFILE}&pagesize=100\"\n",
    ")\n",
    "soup = bs4.BeautifulSoup(html)\n",
    "paper_list: bs4.ResultSet[bs4.Tag] = soup.find_all(\n",
    "    name=\"tr\", attrs={\"class\": \"gsc_a_tr\"}\n",
    ")\n",
    "\n",
    "print(len(paper_list))\n",
    "for paper in paper_list:\n",
    "    paper_link = paper.find(name=\"a\", attrs={\"class\": \"gsc_a_at\"})\n",
    "    print(\n",
    "        \"url =\",\n",
    "        \"https://scholar.google.com\" + paper_link.attrs[\"href\"],\n",
    "        \"title =\",\n",
    "        paper_link.text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get paper cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_soup2dict(paper_html: str) -> typing.Dict[str, bs4.Tag]:\n",
    "    paper_soup = bs4.BeautifulSoup(paper_html)\n",
    "\n",
    "    key_list: bs4.ResultSet[bs4.Tag] = paper_soup.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_field\"}\n",
    "    )\n",
    "    value_list: bs4.ResultSet[bs4.Tag] = paper_soup.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_value\"}\n",
    "    )\n",
    "\n",
    "    paper_dict: typing.Dict[str, bs4.Tag] = {}\n",
    "    paper_dict[\"Title\"] = paper_soup.find(name=\"div\", attrs={\"id\": \"gsc_oci_title\"})\n",
    "    for key, value in zip(key_list, value_list):\n",
    "        paper_dict[key.text] = value\n",
    "\n",
    "    return paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Y0pCki6q_DkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:2osOgNQ5qMEC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:bnK-pcrLprsC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Se3iqnhoufwC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:IjCSPb-OGe4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:8k81kl-MbHgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tyk-4Ss8FVUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:UebtZRa9Y70C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:MXK_kJrjxJIC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:kNdYIx-mwKoC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:YsMSGLbcyi4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:KlAtU1dfN6UC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:9ZlFYXVOiuMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RHpTSmoSYBkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ULOm3_A8WrAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:TFP_iSt0sucC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:iH-uZ7U-co4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:GnPB-g6toBAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:0EnyYjriUFMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:zYLM7Y9cAGgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:nb7KW1ujOQ8C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4TOpqqG69KYC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:eQOLeE2rZwMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ldfaerwXgEUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RGFaLdJalmkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:BqipwSGYUEgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ufrVoPGSRksC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:LkGwnXOMwfcC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:7PzlFSSx8tAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:qjMakFHDy7sC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:35N4QoGY0k4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:NaGl4SEjCO4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ZHo1McVdvXMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pyW8ca7W8N0C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:J_g5lzvAfSwC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:yD5IFk8b50cC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:_xSYboBqXhAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:d1gkVwhDpl0C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4OULZ7Gr8RgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:p2g8aNsByqUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tiz5es2fbqcC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:EUQCXRtRnyEC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:HoB7MX3m0LUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:V3AGJWp-ZtQC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:geHnlv5EZngC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:dshw04ExmUIC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pqnbT2bcN3wC status = 200\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = []\n",
    "for paper in paper_list:\n",
    "    paper_url = (\n",
    "        \"https://scholar.google.com\"\n",
    "        + paper.find(name=\"a\", attrs={\"class\": \"gsc_a_at\"}).attrs[\"href\"]\n",
    "    )\n",
    "    paper_html = get(paper_url)\n",
    "    paper_dict = paper_soup2dict(paper_html)\n",
    "    paper_dict_list.append(paper_dict)\n",
    "pickle.dump(paper_dict_list, open(\"papers.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get paper versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Title', 'Authors', 'Publication date', 'Journal', 'Volume', 'Issue', 'Pages', 'Publisher', 'Description', 'Total citations', 'Scholar articles'])\n",
      "0 Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "1 Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "2 Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "3 From local SGD to local fixed-point methods for federated learning\n",
      "4 RSN: randomized subspace Newton\n",
      "5 Linearly converging error compensated SGD\n",
      "6 Revisiting stochastic extragradient\n",
      "7 Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "8 A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "9 Accelerated methods for saddle-point problem\n",
      "10 Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "11 Decentralized distributed optimization for saddle point problems\n",
      "12 Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "13 Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "14 ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "15 Optimal algorithms for decentralized stochastic variational inequalities\n",
      "16 On accelerated methods for saddle-point problems with composite structure\n",
      "17 The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "18 Towards accelerated rates for distributed optimization over time-varying networks\n",
      "19 Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "20 IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "21 An optimal algorithm for strongly convex minimization under affine constraints\n",
      "22 Distributed fixed point methods with compressed iterates\n",
      "23 Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "24 Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "25 The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "26 Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "27 Fast linear convergence of randomized BFGS\n",
      "28 Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "29 Stochastic spectral and conjugate descent methods\n",
      "30 Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "31 Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "32 Decentralized optimization over time-varying graphs: a survey\n",
      "33 Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "34 On scaled methods for saddle point problems\n",
      "35 Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "36 Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "37 A hypothesis about the rate of global convergence for optimal methods (Newtonâ€™s type) in smooth convex optimization\n",
      "38 An optimal algorithm for strongly convex min-min optimization\n",
      "39 Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "40 Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "41 Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "42 Accelerated variance-reduced methods for saddle-point problems\n",
      "43 Decentralized Optimization with Coupled Constraints\n",
      "44 Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "45 Decentralized Convex Optimization over Time-Varying Graphs\n",
      "46 Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = pickle.load(\n",
    "    open(\"papers.txt\", \"rb\")\n",
    ")\n",
    "print(paper_dict_list[0].keys())\n",
    "for i in range(len(paper_dict_list)):\n",
    "    print(i, paper_dict_list[i][\"Title\"].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_version_urls(scholar_articles: bs4.Tag) -> typing.List[str]:\n",
    "    merged_snippet_list: bs4.ResultSet[bs4.Tag] = scholar_articles.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_merged_snippet\"}\n",
    "    )\n",
    "\n",
    "    version_url_list: typing.List[str] = []\n",
    "    for merged_snippet in merged_snippet_list:\n",
    "        version_href = merged_snippet.find(name=\"a\").attrs[\"href\"]\n",
    "        version_cluster = urllib.parse.parse_qs(\n",
    "            urllib.parse.urlparse(version_href).query\n",
    "        )[\"cluster\"][0]\n",
    "        version_url = f\"https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster={version_cluster}\"\n",
    "        version_url_list.append(version_url)\n",
    "    return version_url_list\n",
    "\n",
    "\n",
    "def extract_versions(scholar_articles: bs4.Tag) -> typing.List[str]:\n",
    "    url_list = extract_version_urls(scholar_articles)\n",
    "\n",
    "    version_list: typing.List[str] = []\n",
    "    for url in url_list:\n",
    "        soup = bs4.BeautifulSoup(get(url))\n",
    "        versions: bs4.ResultSet[bs4.Tag] = soup.find_all(attrs={\"class\": \"gs_rt\"})\n",
    "        for version in versions:\n",
    "            a = version.find(name=\"a\")\n",
    "            if not a is None:\n",
    "                version_list.append(a.attrs[\"href\"])\n",
    "    return version_list\n",
    "\n",
    "\n",
    "# extract_versions(paper_dict_list[9][\"Scholar articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=10432066948921138844 status = 200\n",
      "Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=7006814588298832736 status = 200\n",
      "Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=1371688885190462102 status = 200\n",
      "From local SGD to local fixed-point methods for federated learning\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15088076385024246021 status = 200\n",
      "RSN: randomized subspace Newton\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=7373789986788030402 status = 200\n",
      "Linearly converging error compensated SGD\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9254067822190880000 status = 200\n",
      "Revisiting stochastic extragradient\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=8653605232017703121 status = 200\n",
      "Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15665175903150090179 status = 200\n",
      "A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3790088210576421356 status = 200\n",
      "Accelerated methods for saddle-point problem\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9373209880233203502 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=4686524615161672519 status = 200\n",
      "Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3002705566416746554 status = 200\n",
      "Decentralized distributed optimization for saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15155949253390165076 status = 200\n",
      "Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=218386955491957145 status = 200\n",
      "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=313086167555561841 status = 200\n",
      "ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=13055424477930940340 status = 200\n",
      "Optimal algorithms for decentralized stochastic variational inequalities\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=16974920721308888306 status = 200\n",
      "On accelerated methods for saddle-point problems with composite structure\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12130567569437016761 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=11037264009171741499 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=1257530239269417616 status = 200\n",
      "The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12882769054337015160 status = 200\n",
      "Towards accelerated rates for distributed optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=2324971111724151487 status = 200\n",
      "Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=8964049524700423512 status = 200\n",
      "IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=16969044896100418296 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15707566150174104807 status = 200\n",
      "An optimal algorithm for strongly convex minimization under affine constraints\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15110157264304090339 status = 200\n",
      "Distributed fixed point methods with compressed iterates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9469053425606975651 status = 200\n",
      "Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=6416508855679529689 status = 200\n",
      "Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15617746434035789497 status = 200\n",
      "The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=6878778451294198426 status = 200\n",
      "Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=14069246331553141070 status = 200\n",
      "Fast linear convergence of randomized BFGS\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12043257693440176889 status = 200\n",
      "Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=4252407442233130977 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=18367348657283013424 status = 200\n",
      "Stochastic spectral and conjugate descent methods\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=15120261814030278283 status = 200\n",
      "Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=10468598298336511368 status = 200\n",
      "Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9024856644395226101 status = 200\n",
      "Decentralized optimization over time-varying graphs: a survey\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=12275752593080095457 status = 200\n",
      "Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9454741829138024660 status = 200\n",
      "On scaled methods for saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=6878525993176263563 status = 200\n",
      "Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3040641654477328482 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=4942379545739387070 status = 200\n",
      "Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3502687140939495832 status = 200\n",
      "A hypothesis about the rate of global convergence for optimal methods (Newtonâ€™s type) in smooth convex optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3812646967675013786 status = 200\n",
      "An optimal algorithm for strongly convex min-min optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=13028606441871676013 status = 200\n",
      "Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=10357839628968722372 status = 200\n",
      "Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=9027699018424793418 status = 200\n",
      "Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=3365804316294644363 status = 200\n",
      "Accelerated variance-reduced methods for saddle-point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=14409392578980627652 status = 200\n",
      "Decentralized Optimization with Coupled Constraints\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=13797128321053821676 status = 200\n",
      "Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=11605182395180550612 status = 200\n",
      "Decentralized Convex Optimization over Time-Varying Graphs\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=14256695056462406414 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=16900672409194605727 status = 200\n",
      "Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster=7226585304015966991 status = 200\n"
     ]
    }
   ],
   "source": [
    "for paper_dict in paper_dict_list:\n",
    "    print(paper_dict[\"Title\"].text)\n",
    "    if \"Scholar articles\" in paper_dict.keys():\n",
    "        version_list = extract_versions(paper_dict[\"Scholar articles\"])\n",
    "        paper_dict[\"Version urls\"] = version_list\n",
    "    else:\n",
    "        paper_dict[\"Version urls\"] = []\n",
    "\n",
    "pickle.dump(paper_dict_list, open(\"paper_urls.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Title', 'Authors', 'Publication date', 'Journal', 'Volume', 'Issue', 'Pages', 'Publisher', 'Description', 'Total citations', 'Scholar articles', 'Version urls'])\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = pickle.load(\n",
    "    open(\"paper_urls.txt\", \"rb\")\n",
    ")\n",
    "print(paper_dict_list[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Paper title=\"Stochastic distributed...\" authors=\"Samuel HorvÃ¡th, Dmitry...\" venue=\"Optimization Methods and Software\" year = 2023>\n",
      "<Paper title=\"Donâ€™t jump through hoops...\" authors=\"Dmitry Kovalev, Samuel...\" venue=\"Algorithmic Learning Theory\" year = 2020>\n",
      "<Paper title=\"Acceleration for compressed...\" authors=\"Zhize Li, Dmitry Kovalev,...\" venue=\"International Conference on Machine Learning\" year = 2020>\n",
      "<Paper title=\"From local SGD to local...\" authors=\"Grigory Malinovskiy, Dmitry...\" venue=\"International Conference on Machine Learning\" year = 2020>\n",
      "<Paper title=\"RSN: randomized subspace...\" authors=\"Robert Gower, Dmitry...\" venue=\"Advances in Neural Information Processing Systems\" year = 2019>\n",
      "<Paper title=\"Linearly converging error...\" authors=\"Eduard Gorbunov, Dmitry...\" venue=\"Advances in Neural Information Processing Systems\" year = 2020>\n",
      "<Paper title=\"Revisiting stochastic...\" authors=\"Konstantin Mishchenko,...\" venue=\"International Conference on Artificial Intelligence and Statistics\" year = 2020>\n",
      "<Paper title=\"Optimal and practical...\" authors=\"Dmitry Kovalev, Adil Salim,...\" venue=\"Advances in Neural Information Processing Systems\" year = 2020>\n",
      "<Paper title=\"A linearly convergent...\" authors=\"Dmitry Kovalev, Anastasia...\" venue=\"International Conference on Artificial Intelligence and Statistics\" year = 2021>\n",
      "<Paper title=\"Accelerated methods for...\" authors=\"Mohammad S Alkousa,...\" venue=\"Computational Mathematics and Mathematical Physics\" year = 2020>\n",
      "<Paper title=\"Stochastic Newton and cubic...\" authors=\"Dmitry Kovalev, Konstantin...\" venue=\"arXiv preprint arXiv:1912.01597\" year = 2019>\n",
      "<Paper title=\"Decentralized distributed...\" authors=\"Alexander Rogozin,...\" venue=\"arXiv preprint arXiv:2102.07758\" year = 2021>\n",
      "<Paper title=\"Lower bounds and optimal...\" authors=\"Dmitry Kovalev, Elnur...\" venue=\"Advances in Neural Information Processing Systems\" year = 2021>\n",
      "<Paper title=\"Accelerated primal-dual...\" authors=\"Dmitry Kovalev, Alexander...\" venue=\"Advances in Neural Information Processing Systems\" year = 2022>\n",
      "<Paper title=\"ADOM: accelerated...\" authors=\"Dmitry Kovalev, Egor...\" venue=\"International Conference on Machine Learning\" year = 2021>\n",
      "<Paper title=\"Optimal algorithms for...\" authors=\"Dmitry Kovalev, Aleksandr...\" venue=\"Advances in Neural Information Processing Systems\" year = 2022>\n",
      "<Paper title=\"On accelerated methods for...\" authors=\"Vladislav Tominin, Yaroslav...\" venue=\"arXiv preprint arXiv:2103.09344\" year = 2021>\n",
      "<Paper title=\"The first optimal...\" authors=\"Dmitry Kovalev, Alexander...\" venue=\"Advances in Neural Information Processing Systems\" year = 2022>\n",
      "<Paper title=\"Towards accelerated rates...\" authors=\"Alexander Rogozin,...\" venue=\"Optimization and Applications: 12th International Conference, OPTIMA 2021, Petrovac, Montenegro, September 27â€“October 1, 2021, Proceedings 12\" year = 2021>\n",
      "<Paper title=\"Stochastic proximal...\" authors=\"Adil Salim, Dmitry Kovalev,...\" venue=\"Advances in Neural Information Processing Systems\" year = 2019>\n",
      "<Paper title=\"IntSGD: Adaptive floatless...\" authors=\"Konstantin Mishchenko,...\" venue=\"International Conference on Learning Representations\" year = 2022>\n",
      "<Paper title=\"An optimal algorithm for...\" authors=\"Adil Salim, Laurent Condat,...\" venue=\"International conference on artificial intelligence and statistics\" year = 2022>\n",
      "<Paper title=\"Distributed fixed point...\" authors=\"SÃ©lim Chraibi, Ahmed...\" venue=\"arXiv preprint arXiv:1912.09925\" year = 2019>\n",
      "<Paper title=\"Smooth monotone stochastic...\" authors=\"Aleksandr Beznosikov, Boris...\" venue=\"European Mathematical Society Magazine\" year = 2023>\n",
      "<Paper title=\"Optimal gradient sliding...\" authors=\"Dmitry Kovalev, Aleksandr...\" venue=\"Advances in Neural Information Processing Systems\" year = 2022>\n",
      "<Paper title=\"The first optimal algorithm...\" authors=\"Dmitry Kovalev, Alexander...\" venue=\"Advances in Neural Information Processing Systems\" year = 2022>\n",
      "<Paper title=\"Variance reduced coordinate...\" authors=\"Filip Hanzely, Dmitry...\" venue=\"International Conference on Machine Learning\" year = 2020>\n",
      "<Paper title=\"Fast linear convergence of...\" authors=\"Dmitry Kovalev, Robert M...\" venue=\"arXiv preprint arXiv:2002.11337\" year = 2020>\n",
      "<Paper title=\"Near-optimal decentralized...\" authors=\"Aleksandr Beznosikov,...\" venue=\"Optimization and Applications: 12th International Conference, OPTIMA 2021, Petrovac, Montenegro, September 27â€“October 1, 2021, Proceedings 12\" year = 2021>\n",
      "<Paper title=\"Stochastic spectral and...\" authors=\"Dmitry Kovalev, Peter...\" venue=\"Advances in Neural Information Processing Systems\" year = 2018>\n",
      "<Paper title=\"Communication acceleration...\" authors=\"Abdurakhmon Sadiev, Dmitry...\" venue=\"Advances in Neural Information Processing Systems\" year = 2022>\n",
      "<Paper title=\"Decentralized saddle-point...\" authors=\"Dmitry Metelev, Alexander...\" venue=\"Computational Management Science\" year = 2024>\n",
      "<Paper title=\"Decentralized optimization...\" authors=\"Alexander Rogozin,...\" venue=\"arXiv preprint arXiv:2210.09719\" year = 2022>\n",
      "<Paper title=\"Is consensus acceleration...\" authors=\"Dmitry Metelev, Alexander...\" venue=\"International Conference on Machine Learning\" year = 2023>\n",
      "<Paper title=\"On scaled methods for...\" authors=\"Aleksandr Beznosikov, Aibek...\" venue=\"arXiv preprint arXiv:2206.08303\" year = 2022>\n",
      "<Paper title=\"Decentralized convex...\" authors=\"Olga Yufereva, Michael...\" venue=\"Computational Management Science\" year = 2024>\n",
      "<Paper title=\"Non-smooth setting of...\" authors=\"Aleksandr Lobanov, Andrew...\" venue=\"Computational Management Science\" year = 2023>\n",
      "<Paper title=\"A hypothesis about the rate...\" authors=\"Alexander Gasnikov, Dmitry...\" venue=\"Computer research and modeling\" year = 2018>\n",
      "<Paper title=\"An optimal algorithm for...\" authors=\"Alexander Gasnikov, Dmitry...\" venue=\"arXiv preprint arXiv:2212.14439\" year = 2022>\n",
      "<Paper title=\"Decentralized saddle point...\" authors=\"Alexander Rogozin,...\" venue=\"Optimization Methods and Software\" year = 2024>\n",
      "<Paper title=\"Decentralized Finite-Sum...\" authors=\"Dmitry Metelev, Savelii...\" venue=\"arXiv preprint arXiv:2402.02490\" year = 2024>\n",
      "<Paper title=\"Optimal algorithm with...\" authors=\"Ekaterina Borodich, Georgiy...\" venue=\"arXiv preprint arXiv:2307.12946\" year = 2023>\n",
      "<Paper title=\"Accelerated variance-...\" authors=\"Ekaterina Borodich,...\" venue=\"EURO Journal on Computational Optimization\" year = 2022>\n",
      "<Paper title=\"Decentralized Optimization...\" authors=\"Demyan Yarmoshik, Dmitry...\" venue=\"arXiv preprint arXiv:2407.02020\" year = 2024>\n",
      "<Paper title=\"Lower Bounds and Optimal...\" authors=\"Dmitry Kovalev, Ekaterina...\" venue=\"arXiv preprint arXiv:2405.18031\" year = 2024>\n",
      "<Paper title=\"Decentralized Convex...\" authors=\"Alexander Rogozin,...\" venue=\"Encyclopedia of Optimization\" year = 2023>\n",
      "<Paper title=\"Optimal Algorithms for...\" authors=\"Dmitry Kovalev\" venue=\"King Abdullah University of Science and Technology\" year = 2022>\n"
     ]
    }
   ],
   "source": [
    "class Paper:\n",
    "    def __init__(self, paper_dict: typing.Dict[str, bs4.Tag]) -> None:\n",
    "        self.title: str = paper_dict[\"Title\"].text\n",
    "        self.authors: str = paper_dict[\"Authors\"].text\n",
    "        self.venue: str = paper_dict.get(\n",
    "            \"Journal\",\n",
    "            paper_dict.get(\n",
    "                \"Conference\",\n",
    "                paper_dict.get(\n",
    "                    \"Book\",\n",
    "                    paper_dict.get(\"Institution\", bs4.Tag(name=\"div\")),\n",
    "                ),\n",
    "            ),\n",
    "        ).text\n",
    "        self.year: str = str(dateparser.parse(paper_dict[\"Publication date\"].text).year)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # return str(self.year) + \" \" + self.title\n",
    "        return '<Paper title=\"{}\" authors=\"{}\" venue=\"{}\" year = {}>'.format(\n",
    "            textwrap.shorten(self.title, width=30, placeholder=\"...\"),\n",
    "            textwrap.shorten(self.authors, width=30, placeholder=\"...\"),\n",
    "            self.venue,\n",
    "            self.year,\n",
    "        )\n",
    "\n",
    "\n",
    "for paper_dict in paper_dict_list:\n",
    "    print(Paper(paper_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for paper_dict in paper_dict_list:\n",
    "#     print(paper_dict[\"Title\"].text)\n",
    "#     for url in paper_dict[\"Version urls\"]:\n",
    "#         print(\"\\t\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
