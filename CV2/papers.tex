\section{Publications}
\begin{enumerate}
\item \textbf{Near-Optimal Decentralized Algorithms for Saddle Point Problems over Time-Varying Networks} (\href{https://anbeznosikov.github.io}{\color{linkcolour}Aleksandr Beznosikov}, \href{https://scholar.google.com/citations?user=sEjyzkgAAAAJ}{\color{linkcolour}Alexander Rogozin}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18}{\em \color{black}OPTIMA 2021}
\item \textbf{Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://elnurgasanov.com}{\color{linkcolour}Elnur Gasanov}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html}{\em \color{black}NeurIPS 2021}
\item \textbf{An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints} (\href{https://adil-salim.github.io}{\color{linkcolour}Adil Salim}, \href{https://lcondat.github.io}{\color{linkcolour}Laurent Condat}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{http://aistats.org/aistats2022/accepted.html}{\em \color{black}AISTATS 2022}
\item \textbf{ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://shulgin-egor.github.io}{\color{linkcolour}Egor Shulgin}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://scholar.google.com/citations?user=sEjyzkgAAAAJ}{\color{linkcolour}Alexander Rogozin}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{http://proceedings.mlr.press/v139/kovalev21a}{\em \color{black}ICML 2021}
\item \textbf{IntSGD: Floatless Compression of Stochastic Gradients} (\href{https://konstmish.github.io}{\color{linkcolour}Konstantin Mishchenko}, \href{https://bokunwang1.github.io}{\color{linkcolour}Bokun Wang}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://openreview.net/forum?id=pFyXqxChZc}{\em \color{black}ICLR 2022}
\item \textbf{A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://scholar.google.com/citations?user=ldJpvE8AAAAJ}{\color{linkcolour}Anastasia Koloskova}, \href{https://people.epfl.ch/martin.jaggi}{\color{linkcolour}Martin Jaggi}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://sstich.ch}{\color{linkcolour}Sebastian U. Stich}), \href{http://proceedings.mlr.press/v130/kovalev21a}{\em \color{black}AISTATS 2021}
\item \textbf{Linearly Converging Error Compensated SGD} (\href{https://eduardgorbunov.github.io}{\color{linkcolour}Eduard Gorbunov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{}{\color{linkcolour}Dmitry Makarenko}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://papers.nips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html}{\em \color{black}NeurIPS 2020}
\item \textbf{Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://adil-salim.github.io}{\color{linkcolour}Adil Salim}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://papers.nips.cc/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html}{\em \color{black}NeurIPS 2020}
\item \textbf{From Local SGD to Local Fixed Point Methods for Federated Learning} (\href{https://scholar.google.com/citations?user=4w2W9KQAAAAJ}{\color{linkcolour}Grigory Malinovsky}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://elnurgasanov.com}{\color{linkcolour}Elnur Gasanov}, \href{https://lcondat.github.io}{\color{linkcolour}Laurent Condat}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{http://proceedings.mlr.press/v119/malinovskiy20a.html}{\em \color{black}ICML 2020}
\item \textbf{Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization} (\href{https://zhizeli.github.io}{\color{linkcolour}Zhize Li}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://qianxunk.github.io}{\color{linkcolour}Xun Qian}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{http://proceedings.mlr.press/v119/li20g.html}{\em \color{black}ICML 2020}
\item \textbf{Variance Reduced Coordinate Descent with Acceleration: New Method With a Surprising Application to Finite-Sum Problems} (\href{https://fhanzely.github.io/index.html}{\color{linkcolour}Filip Hanzely}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{http://proceedings.mlr.press/v119/hanzely20b.html}{\em \color{black}ICML 2020}
\item \textbf{Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://konstmish.github.io}{\color{linkcolour}Konstantin Mishchenko}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://sites.google.com/site/optneurips19/}{\em \color{black}NeurIPS 2019 Workshop}
\item \textbf{Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates} (\href{https://adil-salim.github.io}{\color{linkcolour}Adil Salim}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://papers.nips.cc/paper/8891-stochastic-proximal-langevin-algorithm-potential-splitting-and-nonasymptotic-rates}{\em \color{black}NeurIPS 2019}
\item \textbf{Revisiting Stochastic Extragradient} (\href{https://konstmish.github.io}{\color{linkcolour}Konstantin Mishchenko}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://shulgin-egor.github.io}{\color{linkcolour}Egor Shulgin}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://scholar.google.com/citations?user=GI_-KjoAAAAJ}{\color{linkcolour}Yura Malitsky}), \href{http://proceedings.mlr.press/v108/mishchenko20a}{\em \color{black}AISTATS 2020}
\item \textbf{RSN: Randomized Subspace Newton} (\href{https://gowerrobert.github.io}{\color{linkcolour}Robert M. Gower}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{http://www.opt.uni-duesseldorf.de/~lieder/de/inhalt.php}{\color{linkcolour}Felix Lieder}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://papers.nips.cc/paper/8351-rsn-randomized-subspace-newton}{\em \color{black}NeurIPS 2019}
\item \textbf{Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://samuelhorvath.github.io}{\color{linkcolour}Samuel Horvath}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{http://proceedings.mlr.press/v117/kovalev20a}{\em \color{black}ALT 2020}
\item \textbf{A hypothesis about the rate of global convergence for optimal methods (Newton's type) in smooth convex optimization} (\href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}), \href{http://crm-en.ics.org.ru/journal/article/2685/}{\em \color{black}Computer Research and Modeling}
\item \textbf{Stochastic Spectral and Conjugate Descent Methods} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://eduardgorbunov.github.io}{\color{linkcolour}Eduard Gorbunov}, \href{https://elnurgasanov.com}{\color{linkcolour}Elnur Gasanov}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://papers.nips.cc/paper/7596-stochastic-spectral-and-conjugate-descent-methods}{\em \color{black}NeurIPS 2018}
\end{enumerate}
\section{Preprints}
\begin{enumerate}
\item \textbf{On Scaled Methods for Saddle Point Problems} (\href{https://anbeznosikov.github.io}{\color{linkcolour}Aleksandr Beznosikov}, \href{}{\color{linkcolour}Aibek Alanov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://mtakac.com}{\color{linkcolour}Martin Takac}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://arxiv.org/abs/2206.08303}{\em \color{black}arXiv preprint (June 2022)}
\item \textbf{Decentralized Saddle-Point Problems with Different Constants of Strong Convexity and Strong Concavity} (\href{}{\color{linkcolour}Dmitriy Metelev}, \href{https://scholar.google.com/citations?user=sEjyzkgAAAAJ}{\color{linkcolour}Alexander Rogozin}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}), \href{https://arxiv.org/abs/2206.00090}{\em \color{black}arXiv preprint (May 2022)}
\item \textbf{Decentralized Computation of Wasserstein Barycenter over Time-Varying Networks} (\href{https://scholar.google.com/citations?user=v6tuw7IAAAAJ}{\color{linkcolour}Olga Yufereva}, \href{}{\color{linkcolour}Michael Persiianov}, \href{http://wias-berlin.de/people/dvureche/}{\color{linkcolour}Pavel Dvurechensky}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}), \href{https://arxiv.org/abs/2205.15669}{\em \color{black}arXiv preprint (May 2022)}
\item \textbf{Optimal Gradient Sliding and its Application to Distributed Optimization Under Similarity} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://anbeznosikov.github.io}{\color{linkcolour}Aleksandr Beznosikov}, \href{https://scholar.google.com/citations?user=9Dapoy8AAAAJ}{\color{linkcolour}Ekaterina Borodich}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://engineering.purdue.edu/~gscutari/}{\color{linkcolour}Gesualdo Scutari}), \href{https://arxiv.org/abs/2205.15136}{\em \color{black}arXiv preprint (May 2022)}
\item \textbf{The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://arxiv.org/abs/2205.09647}{\em \color{black}arXiv preprint (May 2022)}
\item \textbf{The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://arxiv.org/abs/2205.05653}{\em \color{black}arXiv preprint (May 2022)}
\item \textbf{Optimal Algorithms for Decentralized Stochastic Variational Inequalities} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://anbeznosikov.github.io}{\color{linkcolour}Aleksandr Beznosikov}, \href{https://scholar.google.com/citations?user=R-xZRIAAAAAJ&hl=ru}{\color{linkcolour}Abdurakhmon Sadiev}, \href{}{\color{linkcolour}Michael Persiianov}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://arxiv.org/abs/2202.02771}{\em \color{black}arXiv preprint (February 2022)}
\item \textbf{Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}), \href{https://arxiv.org/abs/2112.15199}{\em \color{black}arXiv preprint (December 2021)}
\item \textbf{Decentralized Distributed Optimization for Saddle Point Problems} (\href{https://scholar.google.com/citations?user=sEjyzkgAAAAJ}{\color{linkcolour}Alexander Rogozin}, \href{https://anbeznosikov.github.io}{\color{linkcolour}Alexander Beznosikov}, \href{https://scholar.google.com/citations?user=5ILnTRsAAAAJ}{\color{linkcolour}Darina Dvinskikh}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{http://wias-berlin.de/people/dvureche/}{\color{linkcolour}Pavel Dvurechensky}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}), \href{https://arxiv.org/abs/2102.07758}{\em \color{black}arXiv preprint (February 2021)}
\item \textbf{Towards Accelerated Rates for Distributed Optimization over Time-varying Networks} (\href{https://scholar.google.com/citations?user=sEjyzkgAAAAJ}{\color{linkcolour}Alexander Rogozin}, \href{}{\color{linkcolour}Vladislav Lukoshkin}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://shulgin-egor.github.io}{\color{linkcolour}Egor Shulgin}), \href{https://arxiv.org/abs/2009.11069}{\em \color{black}arXiv preprint (September 2020)}
\item \textbf{Fast Linear Convergence of Randomized BFGS} (\href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://gowerrobert.github.io}{\color{linkcolour}Robert M. Gower}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://scholar.google.com/citations?user=sEjyzkgAAAAJ}{\color{linkcolour}Alexander Rogozin}), \href{https://arxiv.org/abs/2002.11337}{\em \color{black}arXiv preprint (February 2020)}
\item \textbf{Distributed Fixed Point Methods with Compressed Iterates} (\href{https://scholar.google.com/citations?user=gyiubRkAAAAJ}{\color{linkcolour}Selim Chraibi}, \href{https://rka97.github.io}{\color{linkcolour}Ahmed Khaled}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://adil-salim.github.io}{\color{linkcolour}Adil Salim}, \href{https://mtakac.com}{\color{linkcolour}Martin Takac}), \href{https://arxiv.org/abs/1912.09925}{\em \color{black}arXiv preprint (December 2019)}
\item \textbf{Accelerated methods for composite non-bilinear saddle point problem} (\href{https://scholar.google.com/citations?user=dJgWojUAAAAJ}{\color{linkcolour}Mohammad Alkousa}, \href{https://scholar.google.com/citations?user=5ILnTRsAAAAJ}{\color{linkcolour}Darina Dvinskikh}, \href{https://www.researchgate.net/profile/Fedor_Stonyakin}{\color{linkcolour}Fedor Stonyakin}, \href{https://scholar.google.ru/citations?user=AmeE8qkAAAAJ}{\color{linkcolour}Alexander Gasnikov}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}), \href{https://arxiv.org/abs/1906.03620}{\em \color{black}arXiv preprint (December 2019)}
\item \textbf{Stochastic Distributed Learning with Gradient Quantization and Variance Reduction} (\href{https://samuelhorvath.github.io}{\color{linkcolour}Samuel Horvath}, \href{https://www.dmitry-kovalev.com}{\color{linkcolour}Dmitry Kovalev}, \href{https://konstmish.github.io}{\color{linkcolour}Konstantin Mishchenko}, \href{https://richtarik.org}{\color{linkcolour}Peter Richtarik}, \href{https://sstich.ch}{\color{linkcolour}Sebastian U. Stich}), \href{https://arxiv.org/abs/1904.05115}{\em \color{black}arXiv preprint (April 2019)}
\end{enumerate}
